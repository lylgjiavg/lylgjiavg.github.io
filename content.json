{"meta":{"title":"Jiavg","subtitle":"LYLG-JLC","description":"个人技术总结","author":"Jiavg","url":"https://lylgjiavg.github.io","root":"/"},"pages":[{"title":"about","date":"2018-09-30T09:25:30.000Z","updated":"2019-06-29T15:25:35.750Z","comments":true,"path":"about/index.html","permalink":"https://lylgjiavg.github.io/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2018-09-30T09:25:30.000Z","updated":"2019-06-29T15:24:29.993Z","comments":true,"path":"categories/index.html","permalink":"https://lylgjiavg.github.io/categories/index.html","excerpt":"","text":""},{"title":"friends","date":"2018-12-12T13:25:30.000Z","updated":"2019-06-29T15:26:00.186Z","comments":true,"path":"friends/index.html","permalink":"https://lylgjiavg.github.io/friends/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-09-30T10:23:38.000Z","updated":"2019-06-29T15:25:03.807Z","comments":true,"path":"tags/index.html","permalink":"https://lylgjiavg.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Elasticsearch篇之倒排索引与分词","slug":"第3章-Elasticsearch篇之倒排索引与分词","date":"2020-04-07T17:24:47.000Z","updated":"2020-04-07T21:15:43.312Z","comments":true,"path":"2020/04/08/第3章-Elasticsearch篇之倒排索引与分词/","link":"","permalink":"https://lylgjiavg.github.io/2020/04/08/第3章-Elasticsearch篇之倒排索引与分词/","excerpt":"","text":"Elasticsearch篇之倒排索引与分词书与搜索引擎 目录页对应正排索引 索引页对应倒排索引 搜索引擎 正排索引 文档Id到文档内容, 单词的关联关系 倒排索引 单词到文档Id的关联关系 搜索查询流程结合正排索引和倒排索引实现搜索查询流程: 查询包含”搜索引擎”的文档 通过倒排索引获得”搜索引擎”对应的文档id 有 1 和 3 通过正排索引查询 1 和 3 的完整内容 返回用户最终结果 倒排索引组成 倒排索引是搜索引擎的核心, 主要包含两部分: 单词词典 (Term Dictionary) 倒排列表 (Posting List) 单词词典 单词词典(Term Dictionary)是倒排索引的重要组成 记录所有文档的单词, 一般都比较大 记录单词到倒排列表的关联信息 单词词典的实现一般是用B + Tree, 示例如下图: 下图排序采用拼音实现 倒排列表 倒排列表 (Posting List) 记录了单词对应的文档集合, 由倒排索引项 (Posting) 组成 倒排索引项 (Posting) 主要包含如下信息: 文档Id, 用于获取原始信息 单词频率 (TF, Term Frequency), 记录该单词在文档中的出现次数, 用于后续相关性算分 位置 (Position), 记录单词在文档中的分词位置 (多个), 用于做词语搜索 (Phrase Query) 偏移 (Offset), 记录单词在文档的开始和结束位置, 用于做高亮显示 示例: 以”搜索引擎”为例 倒排索引 单词字典与倒排列表结合在一起的结构如下: es存储的是一个json格式的文档, 其中包含多个字段, 每个字段会有自己的倒排索引, 类似下图: 分词 分词是指将文本转换成一系列单词 (term or token)的过程, 也可以叫做文本分析, 在es里面称为Analysis, 如下图所示: 分词器 分词器是es中专门处理分词的组件, 英文为Analyzer, 它的组成如下: Character Filters 针对原始文本进行处理, 比如去除HTML特殊标记符 Tokenizer (一个分词器只能有一个) 将原始文本按照一定的规则切分为单词 Token Filters 针对tokenizer处理的单词进行再加工, 比如转小写, 删除(Stop words), 或新增(近义词, 同义词)等处理 调用顺序 Analyze API es提供一个测试分词的api接口, 方便验证分词效果, endpoint是_analyze 可以直接指定analyzer进行测试 可以直接指定索引中的字段进行测试 可以自定义分词器进行测试 直接指定analyzer进行测试接口如下: 直接指定索引中的字段进行测试接口如下: 自定义分词器进行测试接口如下: 预定义的分词器 es自带如下的分词器 Standard Simple Whitespace Stop Keyword Pattern Language Standard Analyzer 默认分词器 其组成如图, 特性为: 按词切分, 支持多语言 小写处理 运行示例: # request POST /_analyze { \"analyzer\": \"standard\", \"text\": [\"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"] } # response { \"tokens\": [ { \"token\": \"the\", \"start_offset\": 0, \"end_offset\": 3, \"type\": \"\", \"position\": 0 }, { \"token\": \"2\", \"start_offset\": 4, \"end_offset\": 5, \"type\": \"\", \"position\": 1 }, { \"token\": \"quick\", \"start_offset\": 6, \"end_offset\": 11, \"type\": \"\", \"position\": 2 }, { \"token\": \"brown\", \"start_offset\": 12, \"end_offset\": 17, \"type\": \"\", \"position\": 3 }, { \"token\": \"foxes\", \"start_offset\": 18, \"end_offset\": 23, \"type\": \"\", \"position\": 4 }, { \"token\": \"jumped\", \"start_offset\": 24, \"end_offset\": 30, \"type\": \"\", \"position\": 5 }, { \"token\": \"over\", \"start_offset\": 31, \"end_offset\": 35, \"type\": \"\", \"position\": 6 }, { \"token\": \"the\", \"start_offset\": 36, \"end_offset\": 39, \"type\": \"\", \"position\": 7 }, { \"token\": \"lazy\", \"start_offset\": 40, \"end_offset\": 44, \"type\": \"\", \"position\": 8 }, { \"token\": \"dog's\", \"start_offset\": 45, \"end_offset\": 50, \"type\": \"\", \"position\": 9 }, { \"token\": \"bone\", \"start_offset\": 51, \"end_offset\": 55, \"type\": \"\", \"position\": 10 } ] } Simple Analyzer 其组成如图, 特性为: 按照非字母切分 小写处理 运行示例: # request POST /_analyze { \"analyzer\": \"simple\", \"text\": [\"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"] } # response { \"tokens\": [ { \"token\": \"the\", \"start_offset\": 0, \"end_offset\": 3, \"type\": \"word\", \"position\": 0 }, { \"token\": \"quick\", \"start_offset\": 6, \"end_offset\": 11, \"type\": \"word\", \"position\": 1 }, { \"token\": \"brown\", \"start_offset\": 12, \"end_offset\": 17, \"type\": \"word\", \"position\": 2 }, { \"token\": \"foxes\", \"start_offset\": 18, \"end_offset\": 23, \"type\": \"word\", \"position\": 3 }, { \"token\": \"jumped\", \"start_offset\": 24, \"end_offset\": 30, \"type\": \"word\", \"position\": 4 }, { \"token\": \"over\", \"start_offset\": 31, \"end_offset\": 35, \"type\": \"word\", \"position\": 5 }, { \"token\": \"the\", \"start_offset\": 36, \"end_offset\": 39, \"type\": \"word\", \"position\": 6 }, { \"token\": \"lazy\", \"start_offset\": 40, \"end_offset\": 44, \"type\": \"word\", \"position\": 7 }, { \"token\": \"dog\", \"start_offset\": 45, \"end_offset\": 48, \"type\": \"word\", \"position\": 8 }, { \"token\": \"s\", \"start_offset\": 49, \"end_offset\": 50, \"type\": \"word\", \"position\": 9 }, { \"token\": \"bone\", \"start_offset\": 51, \"end_offset\": 55, \"type\": \"word\", \"position\": 10 } ] } Whitespace Analyzer其组成如图, 特性为: 按照空格切分 运行示例: # request POST /_analyze { \"analyzer\": \"whitespace\", \"text\": [\"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"] } # response { \"tokens\": [ { \"token\": \"The\", \"start_offset\": 0, \"end_offset\": 3, \"type\": \"word\", \"position\": 0 }, { \"token\": \"2\", \"start_offset\": 4, \"end_offset\": 5, \"type\": \"word\", \"position\": 1 }, { \"token\": \"QUICK\", \"start_offset\": 6, \"end_offset\": 11, \"type\": \"word\", \"position\": 2 }, { \"token\": \"Brown-Foxes\", \"start_offset\": 12, \"end_offset\": 23, \"type\": \"word\", \"position\": 3 }, { \"token\": \"jumped\", \"start_offset\": 24, \"end_offset\": 30, \"type\": \"word\", \"position\": 4 }, { \"token\": \"over\", \"start_offset\": 31, \"end_offset\": 35, \"type\": \"word\", \"position\": 5 }, { \"token\": \"the\", \"start_offset\": 36, \"end_offset\": 39, \"type\": \"word\", \"position\": 6 }, { \"token\": \"lazy\", \"start_offset\": 40, \"end_offset\": 44, \"type\": \"word\", \"position\": 7 }, { \"token\": \"dog's\", \"start_offset\": 45, \"end_offset\": 50, \"type\": \"word\", \"position\": 8 }, { \"token\": \"bone.\", \"start_offset\": 51, \"end_offset\": 56, \"type\": \"word\", \"position\": 9 } ] } Stop AnalyzerStop Word指语气助词等修饰性的词语, 比如the, an, 的, 这等等 其组成如图, 特性为: 相比Simple Analyzer多了Stop World处理 运行示例: # request POST /_analyze { \"analyzer\": \"stop\", \"text\": [\"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"] } # response { \"tokens\": [ { \"token\": \"quick\", \"start_offset\": 6, \"end_offset\": 11, \"type\": \"word\", \"position\": 1 }, { \"token\": \"brown\", \"start_offset\": 12, \"end_offset\": 17, \"type\": \"word\", \"position\": 2 }, { \"token\": \"foxes\", \"start_offset\": 18, \"end_offset\": 23, \"type\": \"word\", \"position\": 3 }, { \"token\": \"jumped\", \"start_offset\": 24, \"end_offset\": 30, \"type\": \"word\", \"position\": 4 }, { \"token\": \"over\", \"start_offset\": 31, \"end_offset\": 35, \"type\": \"word\", \"position\": 5 }, { \"token\": \"lazy\", \"start_offset\": 40, \"end_offset\": 44, \"type\": \"word\", \"position\": 7 }, { \"token\": \"dog\", \"start_offset\": 45, \"end_offset\": 48, \"type\": \"word\", \"position\": 8 }, { \"token\": \"s\", \"start_offset\": 49, \"end_offset\": 50, \"type\": \"word\", \"position\": 9 }, { \"token\": \"bone\", \"start_offset\": 51, \"end_offset\": 55, \"type\": \"word\", \"position\": 10 } ] } Keyword Analyzer其组成如图, 特性为: 不分词, 直接将输入作为一个单词输出 运行示例: # request POST /_analyze { \"analyzer\": \"keyword\", \"text\": [\"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"] } # response { \"tokens\": [ { \"token\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\", \"start_offset\": 0, \"end_offset\": 56, \"type\": \"word\", \"position\": 0 } ] } Pattern Analyzer其组成如图, 特性为: 通过正则表达式自定义分隔符 默认是\\W+, 即非单词的符号作为分隔符 运行示例: # request POST /_analyze { \"analyzer\": \"pattern\", \"text\": [\"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"] } # response { \"tokens\": [ { \"token\": \"the\", \"start_offset\": 0, \"end_offset\": 3, \"type\": \"word\", \"position\": 0 }, { \"token\": \"2\", \"start_offset\": 4, \"end_offset\": 5, \"type\": \"word\", \"position\": 1 }, { \"token\": \"quick\", \"start_offset\": 6, \"end_offset\": 11, \"type\": \"word\", \"position\": 2 }, { \"token\": \"brown\", \"start_offset\": 12, \"end_offset\": 17, \"type\": \"word\", \"position\": 3 }, { \"token\": \"foxes\", \"start_offset\": 18, \"end_offset\": 23, \"type\": \"word\", \"position\": 4 }, { \"token\": \"jumped\", \"start_offset\": 24, \"end_offset\": 30, \"type\": \"word\", \"position\": 5 }, { \"token\": \"over\", \"start_offset\": 31, \"end_offset\": 35, \"type\": \"word\", \"position\": 6 }, { \"token\": \"the\", \"start_offset\": 36, \"end_offset\": 39, \"type\": \"word\", \"position\": 7 }, { \"token\": \"lazy\", \"start_offset\": 40, \"end_offset\": 44, \"type\": \"word\", \"position\": 8 }, { \"token\": \"dog\", \"start_offset\": 45, \"end_offset\": 48, \"type\": \"word\", \"position\": 9 }, { \"token\": \"s\", \"start_offset\": 49, \"end_offset\": 50, \"type\": \"word\", \"position\": 10 }, { \"token\": \"bone\", \"start_offset\": 51, \"end_offset\": 55, \"type\": \"word\", \"position\": 11 } ] } Language Analyzer 提供了30+ 常见的分词器 支持的语言有: arabic, armenian, basque, bengali, brazilian, bulgarian, catalan, cjk, czech, danish, dutch, english, finnish, french, galician, german, greek, hindi, hungarian, indonesian, irish, italian, latvian, lithuanian, norwegian, persian, portuguese, romanian, russian, sorani, spanish, swedish, turkish, thai. 中文分词 难点 中文分词指的是将一个汉字序列切分为一个一个单独的词. 在英文中, 单词是以空格作为自然分界符, 汉语中词没有一个形式上的分界符. 上下文不同, 分词结果迥异, 比如交叉歧义问题, 比如下面两种分词都合理 乒乓球拍/卖/完了 乒乓球/拍卖/完了 常用分词系统 IK 实现中英文单词的切分, 支持ik_smart, ik_maxword等模式 可自定义词库, 支持热更新分词词典 Github地址: https://github.com/medcl/elasticsearch-analysis-ik jieba Python中最流行的分词系统, 支持分词和词性标注 支持繁体分词, 自定义词典, 并行分词等 Github地址: https://github.com/sing1ee/elasticsearch-jieba-plugin 基于自然语言处理的分词系统 Hanlp HanLP 是由一系列模型与算法组成的 Java 工具包，目标是普及自然语言处理在生产环境中的应用。 Github地址: https://github.com/hankcs/HanLP THULAC THULAC（THU Lexical Analyzer for Chinese）由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能。 Github地址: https://github.com/microbun/elasticsearch-thulac-plugin 自定义分词 当自带的分词无法满足需求时, 可以自定义分词 通过自定义Character Filters, Tokenizer, Token Filter实现 Character Filters 在Tokenizer之前对原始文本进行处理, 比如增加, 删除,或者替换字符等 自带的如下 HTML Strip去除HTML标签和转换HTML实体 Mapping进行字符替换操作 Pattern Replace进行正则匹配替换 会影响后续Tokenizer解析的position和offset信息 Character Filters测试api # request POST _analyze { \"tokenizer\": \"keyword\", \"char_filter\": [\"html_strip\"], \"text\": [\"I &apos;m so happy!\"] } # response { \"tokens\": [ { \"token\": \"\"\" I 'm so happy ! \"\"\", \"start_offset\": 0, \"end_offset\": 33, \"type\": \"word\", \"position\": 0 } ] } Tokenizer 将原始文本按照一定规则切分为单词 (term or token) 自带的如下: standard 按照单词进行分割 letter 按照非字符累进行分割 whitespace 按照空格进行分割 UAX URL Email 按照standard分割, 但不会分割邮箱和url NGram 和 Edge NGram连词分割 (可以用来做单词提示) Path Hierarchy按照文件路径进行切割 Tokenizer测试api # request POST /_analyze { \"tokenizer\": \"path_hierarchy\", \"text\": [\"/root/jiavg/data\"] } # response { \"tokens\": [ { \"token\": \"/root\", \"start_offset\": 0, \"end_offset\": 5, \"type\": \"word\", \"position\": 0 }, { \"token\": \"/root/jiavg\", \"start_offset\": 0, \"end_offset\": 11, \"type\": \"word\", \"position\": 0 }, { \"token\": \"/root/jiavg/data\", \"start_offset\": 0, \"end_offset\": 16, \"type\": \"word\", \"position\": 0 } ] } Token Filters 对于Tokenizer输出的单词 (term) 进行增加, 删除, 修改等操作 自带的如下 lowercase 将所有term转换为小写 stop 删除 stop word NGram 和 Edge NGram 连词分割 Synonym 添加近义词的term Filter测试api # request POST _analyze { \"tokenizer\": \"standard\", \"filter\": [ \"stop\", \"lowercase\", { \"type\": \"ngram\", \"max_gram\": 4, \"min_gram\": 4 } ], \"text\": [ \"a Hello Jiavg\" ] } # response { \"tokens\": [ { \"token\": \"hell\", \"start_offset\": 2, \"end_offset\": 7, \"type\": \"\", \"position\": 1 }, { \"token\": \"ello\", \"start_offset\": 2, \"end_offset\": 7, \"type\": \"\", \"position\": 1 }, { \"token\": \"jiav\", \"start_offset\": 8, \"end_offset\": 13, \"type\": \"\", \"position\": 2 }, { \"token\": \"iavg\", \"start_offset\": 8, \"end_offset\": 13, \"type\": \"\", \"position\": 2 } ] } 自定义分词API 自定义分词需要在索引的配置中设定, 如下所示: 自定义分词示例 示例一: # request PUT test_index { \"settings\": { \"analysis\": { \"analyzer\": { \"my_analyzer\": { \"type\": \"custom\", \"char_filter\": [\"html_strip\"], \"tokenizer\": \"standard\", \"filter\": [\"lowercase\", \"asciifolding\"] } } } } } # response { \"acknowledged\": true, \"shards_acknowledged\": true, \"index\": \"test_index\" } 示例二: # request PUT test_index2 { \"settings\": { \"analysis\": { \"analyzer\": { \"my_analyzer2\": { \"type\": \"custom\", \"char_filter\": [ \"my_char_filter\" ], \"tokenizer\": \"my_tokenizer\", \"filter\": [ \"my_filter\", \"lowercase\" ] } }, \"char_filter\": { \"my_char_filter\": { \"type\": \"mapping\", \"mappings\": [ \"(- ^ -) => _口亨_\", \"(- - -) => _哭唧唧_\" ] } }, \"tokenizer\": { \"my_tokenizer\": { \"type\": \"pattern\", \"pattern\": \"[.,!?\\\\s]\" } }, \"filter\": { \"my_filter\": { \"type\": \"stop\", \"stopwords\": \"_english_\" } } } } } # response { \"acknowledged\": true, \"shards_acknowledged\": true, \"index\": \"test_index2\" } # request POST test_index2/_analyze { \"analyzer\": \"my_analyzer2\", \"text\": \"(- ^ -), no li you, _哭唧唧_\" } # response { \"tokens\": [ { \"token\": \"_口亨_\", \"start_offset\": 0, \"end_offset\": 7, \"type\": \"word\", \"position\": 0 }, { \"token\": \"li\", \"start_offset\": 12, \"end_offset\": 14, \"type\": \"word\", \"position\": 2 }, { \"token\": \"you\", \"start_offset\": 15, \"end_offset\": 18, \"type\": \"word\", \"position\": 3 }, { \"token\": \"_哭唧唧_\", \"start_offset\": 20, \"end_offset\": 25, \"type\": \"word\", \"position\": 4 } ] } 分词使用说明 分词会在如下两个时机使用: 创建或更新文档时 (Index Time), 会对应的文档进行分词处理 查询时 (Search Time), 会对查询语句进行分词 索引时分词 索引时分词是通过配置Index Mapping中每个字段的analyzer属性实现的,如下: 不指定分词时, 默认使用standard 查询时分词 一般不需要特别指定查询分词器时, 直接使用索引时分词器即可, 否则会出现无法匹配的情况 查询时分词的指定方式有如下几种: 查询的时候通过analyzer指定分词器 通过index mapping设置search_analyzer实现 分词使用建议 明确字段是否需要分词, 不需要分词的字段就将type设置为keyword, 可以节省空间和提高写性能 善用_analyzer API, 查看文档的具体分词结果 动手测试","categories":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://lylgjiavg.github.io/categories/Elastic-Stack/"}],"tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://lylgjiavg.github.io/tags/ElasticSearch/"},{"name":"倒排索引","slug":"倒排索引","permalink":"https://lylgjiavg.github.io/tags/倒排索引/"},{"name":"分词","slug":"分词","permalink":"https://lylgjiavg.github.io/tags/分词/"}]},{"title":"Elasticsearch篇之入门","slug":"第2章-Elasticsearch篇之入门","date":"2020-04-06T15:34:58.000Z","updated":"2020-04-06T15:34:38.710Z","comments":true,"path":"2020/04/06/第2章-Elasticsearch篇之入门/","link":"","permalink":"https://lylgjiavg.github.io/2020/04/06/第2章-Elasticsearch篇之入门/","excerpt":"","text":"Elasticsearch篇之入门常用术语 文档 Document 用户存储在es中的数据文档.(相当于数据库表中的一行) 索引 Index 由具有相同字段的文档列表组成.(相当于数据库中的表, es6.0以后index下的type只能有一个, 且官方声明以后会取消掉type这个概念) 节点 Node 一个Elasticsearch的运行实例, 是集群的构成单元. 集群 Cluster 由一个或多个节点组成, 对外提供服务. 文档 Document Document在es中是一个 Json Object, 由字段(Field)组成, 常见数据类型如下: 字符串: text(进行分词的字符串), keyword(不进行分词的字符串) 数值型: long, integer, short, byte, double, float, half_float, scaled_float 布尔: boolean 日期: date 二进制: binary 范围类型: integer_range, float_range, long_range, double_range, date_range 每个文档有唯一的id标识 可以自行指定 也可以es自动生成 如下所示是一条Nginx日志在ES储存为一条文档(Document), ES对其日志信息进行结构化处理, 包含多个字段(Field), 每个字段的字段名(Field Name)对应一个字段值(Field Value) 文档元数据 Document MetaData 每个Document都有一个文档元数据(Document MetaData), 用于标注文档的相关信息 _index: 文档所在的索引名 _type: 文档所在的类型名 _id: 文章唯一id _uid: 组合id, 由_type和_id组成(6.x中_type不再起作用, 所以在6.x版本中这个字段值和_id一样) _source: 文档的原始Json数据, 可以从这里获取每个字段的内容 _all: 整合所有的字段内容到该字段, 默认禁用(官方不推荐使用) 索引 Index 索引中存储具有相同结构的文档(Document) 每个索引都有自己的mapping定义, 用于定义字段名和类型 一个集群可以有多个索引, 比如: nginx日志存储的时候可以按日期每天生成一个索引来存储, 方便维护 nginx-log-2020-04-03 nginx-log-2020-04-04 nginx-log-2020-04-05 Rest API Elasticsearch集群对外提供RESTful API REST: REpresentational State Transfer (表述性状态转移) URI指定资源, 如Index, Document等 Http Method指明资源操作类型, 如GET, POST, PUT, DELETE等 常用两种交互方式 Curl命令行 Kibana DevTools 索引 Index API es有专门的Index API, 用于创建, 更新, 删除索引配置等 创建索引 PUT /{索引名}示例: # request PUT /test_index # response { \"acknowledged\": true, \"shards_acknowledged\": true, \"index\": \"test_index\" } 查看现有索引 GET /_cat/indices示例: # request GET /_cat/indices # response red open account eIBKm9zfQhOZkW6tC1uyEA 5 1 1 0 5.4kb 5.4kb yellow open test_index XTzQRFtzRqK3B3EfULLrEg 5 1 0 0 1.1kb 1.1kb 删除索引 DELETE /{索引名}示例: # request DELETE /test_index # response { \"acknowledged\": true } 文档 Document API es有专门的Document API 创建文档 (创建文档时, 如果索引不存在, es会自动创建对应的index和type) 指定id创建文档 # 其中类型名在6.x以后无实际作用, 并且将来版本要删除, 在这里可以任意指定, 一般指定无意义的doc PUT /{索引名}/{类型名}/{Id} { # 文档内容 }示例: # request PUT /test_index/doc/1 { \"username\": \"Jiavg\", \"age\": 21 } # response # _version是为了在并行修改文档时, 防止发生错误 { \"_index\": \"test_index\", \"_type\": \"doc\", \"_id\": \"1\", \"_version\": 1, \"result\": \"created\", \"_shards\": { \"total\": 2, \"successful\": 1, \"failed\": 0 }, \"_seq_no\": 0, \"_primary_term\": 1 } - 不指定id创建文档 ``` POST /test_index/doc { # 文档内容 } ``` 示例: ```shell # request POST /test_index/doc { &quot;username&quot;: &quot;jlc&quot;, &quot;age&quot;: 20 } # response # 由于未指定id, es将会生成一个id { &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;doc&quot;, &quot;_id&quot;: &quot;QzXQT3EBkfca6l6Y9SXp&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: { &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 }, &quot;_seq_no&quot;: 0, &quot;_primary_term&quot;: 1 } ``` 查询文档 指定要查询的文档id GET /{索引名}/{类型名}/{id}示例: # request GET /test_index/doc/1 # response # _source 储存了文档的原始数据 # 200 response { \"_index\": \"test_index\", \"_type\": \"doc\", \"_id\": \"1\", \"_version\": 1, \"found\": true, \"_source\": { \"username\": \"Jiavg\", \"age\": 21 } } # 404 response { \"_index\": \"test_index\", \"_type\": \"doc\", \"_id\": \"2\", \"found\": false } - 搜索所有文档, 用到_search ``` # 不含查询条件 (查询所有文档) GET /{索引名}/{文档名}/_search # 包含查询条件 (查询符合条件的所有文档) GET /{索引名}/{文档名}/_search { # 查询条件 } ``` 示例: ```shell # 不含查询条件 (查询所有文档) # request GET /test_index/doc/_search # response # took: 查询花费时间, 单位ms { &quot;took&quot;: 4, &quot;timed_out&quot;: false, &quot;_shards&quot;: { &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 }, &quot;hits&quot;: { &quot;total&quot;: 2, # 符合条件的总文档数 &quot;max_score&quot;: 1, &quot;hits&quot;: [ # 返回的文档详情数据数组, 默认前10个文档 { &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;doc&quot;, &quot;_id&quot;: &quot;QzXQT3EBkfca6l6Y9SXp&quot;, &quot;_score&quot;: 1, # 文档的得分 &quot;_source&quot;: { &quot;username&quot;: &quot;jlc&quot;, &quot;age&quot;: 20 } }, { &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;doc&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: { &quot;username&quot;: &quot;Jiavg&quot;, &quot;age&quot;: 21 } } ] } } # 包含查询条件 (查询符合条件的所有文档) # request GET /test_index/doc/_search { &quot;query&quot;: { &quot;term&quot;: { &quot;_id&quot;: 1 } } } # response { &quot;took&quot;: 23, &quot;timed_out&quot;: false, &quot;_shards&quot;: { &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 }, &quot;hits&quot;: { &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ { &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;doc&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: { &quot;username&quot;: &quot;Jiavg&quot;, &quot;age&quot;: 21 } } ] } } ``` 更新文档 POST /{索引名}/{类型名}/{id} { # 更新文档内容 } 删除文档 DELETE /{索引名}/{类型名}/{id} es允许一次创建多个文档, 从而减少网络传输开销, 提升写入速率 endpoint 为 _bulk, 如下: index和create同为创建文档, 不同的是index在创建文档时, 如果文档id已经存在, 则会覆盖相应的内容, 但是create在创建文档时,如果文档id已经存在, 则会报错。 请求 响应 注意: 在使用 _bulk时,REST API端点为/ _bulk，并且期望使用以下以换行符分隔的JSON（NDJSON）结构： action_and_meta_data\\n optional_source\\n action_and_meta_data\\n optional_source\\n .... action_and_meta_data\\n optional_source\\nNDJSON: ndjson（New-line Delimited JSON）是一个比较新的标准，本身超简单，就是一个.ndjson文件中，每行都是一个传统json对象，当然每个json对象中要去掉原本用于格式化的换行符，而json的string中本身就不允许出现换行符（取而代之的是\\n）. 所以当请求的数据为普通Json时会发生错误. 示例: # NDJSON # request POST _bulk {\"index\":{\"_index\":\"test_index\",\"_type\":\"doc\",\"_id\":1}} {\"username\":\"Jiavg-1\",\"age\":5} {\"update\":{\"_index\":\"test_index\",\"_type\":\"doc\",\"_id\":\"QzXQT3EBkfca6l6Y9SXp\"}} {\"doc\":{\"age\":25}} {\"create\":{\"_index\":\"test_index\",\"_type\":\"doc\",\"_id\":3}} {\"username\":\"znc\",\"age\":22} # response { \"took\": 52, \"errors\": false, \"items\": [ { \"index\": { \"_index\": \"test_index\", \"_type\": \"doc\", \"_id\": \"1\", \"_version\": 3, \"result\": \"updated\", \"_shards\": { \"total\": 2, \"successful\": 1, \"failed\": 0 }, \"_seq_no\": 2, \"_primary_term\": 2, \"status\": 200 } }, { \"update\": { \"_index\": \"test_index\", \"_type\": \"doc\", \"_id\": \"QzXQT3EBkfca6l6Y9SXp\", \"_version\": 2, \"result\": \"updated\", \"_shards\": { \"total\": 2, \"successful\": 1, \"failed\": 0 }, \"_seq_no\": 1, \"_primary_term\": 2, \"status\": 200 } }, { \"create\": { \"_index\": \"test_index\", \"_type\": \"doc\", \"_id\": \"3\", \"_version\": 1, \"result\": \"created\", \"_shards\": { \"total\": 2, \"successful\": 1, \"failed\": 0 }, \"_seq_no\": 0, \"_primary_term\": 2, \"status\": 201 } } ] } # 普通json # request POST _bulk { &quot;index&quot;: { &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;doc&quot;, &quot;_id&quot;: 1 } } { &quot;username&quot;: &quot;Jiavg-1&quot;, &quot;age&quot;: 5 } { &quot;update&quot;: { &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;doc&quot;, &quot;_id&quot;: &quot;QzXQT3EBkfca6l6Y9SXp&quot; } } { &quot;doc&quot;: { &quot;age&quot;: 25 } } { &quot;create&quot;: { &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;doc&quot;, &quot;_id&quot;: 3 } } { &quot;username&quot;: &quot;znc&quot;, &quot;age&quot;: 22 } # response { &quot;error&quot;: { &quot;root_cause&quot;: [ { &quot;type&quot;: &quot;json_e_o_f_exception&quot;, &quot;reason&quot;: &quot;Unexpected end-of-input: expected close marker for Object (start marker at [Source: org.elasticsearch.transport.netty4.ByteBufStreamInput@618ff58; line: 1, column: 1])\\n at [Source: org.elasticsearch.transport.netty4.ByteBufStreamInput@618ff58; line: 1, column: 3]&quot; } ], &quot;type&quot;: &quot;json_e_o_f_exception&quot;, &quot;reason&quot;: &quot;Unexpected end-of-input: expected close marker for Object (start marker at [Source: org.elasticsearch.transport.netty4.ByteBufStreamInput@618ff58; line: 1, column: 1])\\n at [Source: org.elasticsearch.transport.netty4.ByteBufStreamInput@618ff58; line: 1, column: 3]&quot; }, &quot;status&quot;: 500 } ``` json和ndjson区别参考: https://blog.csdn.net/github_38885296/article/details/100915601 es允许一次查询多个文档 endpoint为_mget, 如下:","categories":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://lylgjiavg.github.io/categories/Elastic-Stack/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://lylgjiavg.github.io/tags/Elasticsearch/"}]},{"title":"Elasticsearch Stack概述","slug":"第1章-Elasticsearch Stack概述","date":"2020-04-05T12:23:58.000Z","updated":"2020-04-06T15:33:53.259Z","comments":true,"path":"2020/04/05/第1章-Elasticsearch Stack概述/","link":"","permalink":"https://lylgjiavg.github.io/2020/04/05/第1章-Elasticsearch Stack概述/","excerpt":"","text":"Elasticsearch Stack概述 “Search is something that any application should have!” – Shay Banon (Creator of Elasticsearch) 名词介绍 什么是 ELK Stack？很简单，指的就是 Elastic Stack。 ELK: “ELK”是三个开源项目的首字母缩写，这三个项目分别是：Elasticsearch、Logstash 和 Kibana。 Elasticsearch: Elasticsearch 是一个搜索和分析引擎。 Logstash: Logstash 是服务器端数据处理管道，能够同时从多个来源采集数据，转换数据，然后将数据发送到诸如 Elasticsearch 等“存储库”中。 Kibana: Kibana 则可以让用户在 Elasticsearch 中使用图形和图表对数据进行可视化。 Elastic Stack 就是 ELK Stack，但是更加灵活，可以帮助人们出色完成各项事务。Elastic Stack 是 ELK Stack 的更新换代产品。 Elastic Stack特性 使用门槛低, 开发周期短, 上线快 Hadoop: ############################## 30天 Elastic Stack: ####### 7天 性能好, 查询快, 实时展示结果 对T+1说不: T+1即当天结果必须等一天才能拿到分析数据 扩容方便, 快速支撑增长迅猛的数据 Elastic Stack能够非常方便进行扩容, 以达到TP/PB级别的规模 Elastic Stack组成 ELK Stack 组成: Elasticsearch Logstash Kibana Elastic Stack 组成: Elasticsearch Logstash Kibana Beats Elastic Stack 是 ELK Stack 的更新换代产品, 是一个完备的数据分析工具集合. 部件主要作用: Kibana: 数据探索与可视化分析 Elasticsearch: 数据存储, 查询与分析 Beats, Logstash: 数据收集与处理 Beats, Logstash 作用 数据收集与处理 ELT Extract Transform Load 数据源多样 数据文件, 如日志, Excel等 数据库, 如MySQL, Oracle等 http服务 网络数据 支持自定义扩展, 无限可能 Elastic Stack 应用Elastic Stack 是 ELK Stack 的更新换代产品, 是一个完备的数据分析工具集合, 可以应用于: 搜索引擎 日志分析 指标分析","categories":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://lylgjiavg.github.io/categories/Elastic-Stack/"}],"tags":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://lylgjiavg.github.io/tags/Elastic-Stack/"}]},{"title":"实战：分析Elasticsearch 查询语句","slug":"4.案例实战","date":"2020-04-04T14:28:50.000Z","updated":"2020-04-06T15:32:45.553Z","comments":true,"path":"2020/04/04/4.案例实战/","link":"","permalink":"https://lylgjiavg.github.io/2020/04/04/4.案例实战/","excerpt":"","text":"实战: 分析Elasticsearch 查询语句目标 收集ELasticsearch集群的查询语句 分析查询语句的常用语句,响应时长等 方案 应用Packetbeat + Logstash完成数据收集工作 使用Kibana + Elasticsearch完成数据分析工作 Production Cluster # 生产数据 Elasticsearch http://127.0.0.1:9200 Kibana http://127.0.0.1:5601 Monitoring Cluster # 存储Packetbeat抓取到的Production Cluster产生的数据 Elasticsearch http://127.0.0.1:8200 bin/elasticsearch -Ecluster.name=sniff_search -Ehttp.port=8200 -Epath.data=sniff Kibana http://127.0.0.1:8601 bin/kibana -e http://127.0.0.1:8200 -p 8601 Production 和 Monitoring 不能是一个集群,否则会进入抓包死循环 方案配置信息方案之Logstash配置配置文件(sniff_search.conf) input { beats { port =&gt; 5044 } } filter { if &quot;search&quot; in [request]{ grok { match =&gt; { &quot;request&quot; =&gt; &quot;.*\\n\\{(?&lt;query_body&gt;.*)&quot;} } grok { match =&gt; { &quot;path&quot; =&gt; &quot;\\/(?&lt;index&gt;.*)\\/_search&quot;} } if [index] { } else { mutate { add_field =&gt; { &quot;index&quot; =&gt; &quot;All&quot; } } } mutate { update =&gt; { &quot;query_body&quot; =&gt; &quot;{%{query_body}&quot;}} } # mutate { # remove_field =&gt; [ &quot;[http][response][body]&quot; ] # } } output { #stdout{codec=&gt;rubydebug} if &quot;search&quot; in [request]{ elasticsearch { hosts =&gt; &quot;127.0.0.1:8200&quot; } } }方案之Packetbeat配置配置文件(packetbeat_es.yml) packetbeat.interfaces.device: lo packetbeat.protocols: - type: http ports: [9200] send_request: true include_body_for: [\"application/json\",\"x-www-form-urlencoded\"] output.logstash: hosts: [\"127.0.0.1:5044\"] 实战步骤1.运行Production Cluster 运行Elasticsearch (–Production , port=9200, cluster.name=prod_cluster, path.data=prod_data) [jlc@localhost elasticsearch-6.1.1]$ bin/elasticsearch -Ecluster.name=prod_cluster -Ehttp.port=9200 -Epath.data=prod_data 运行Kibana (–Production , es服务地址 http://127.0.0.1:9200, port=5601) [jlc@localhost kibana-6.1.1-linux-x86_64]$ bin/kibana -e http://127.0.0.1:9200 -p 5601 2.运行Monitoring Cluster 运行Elasticsearch (–Monitoring , port=8200, cluster.name=moni_cluster, path.data=moni_data) [jlc@localhost elasticsearch-6.1.1]$ bin/elasticsearch -Ecluster.name=monit_cluster -Ehttp.port=8200 -Epath.data=monit_data 运行Kibana (–Monitoring, es服务地址 http://127.0.0.1:8200, port=6601) [jlc@localhost kibana-6.1.1-linux-x86_64]$ bin/kibana -e http://127.0.0.1:8200 -p 6601 运行Logstash (–Monitoring) [jlc@localhost logstash-6.1.1]$ bin/logstash -f sniff_search.conf 运行Packetbeat (–Monitoring) [jlc@localhost packetbeat-6.1.1-linux-x86_64]$ sudo ./packetbeat -e -c packetbeat_es.yml -strict.perms=false 3.Kibana分析 1) 在浏览器中打开http://127.0.0.1:5601/(Production), 点击Dev Tools面板, 进入Kibana控制台界面 2) 在Kibana控制台界面输入Elasticsearch Query语句并运行,如下图所示: 3) 在浏览器中打开http://127.0.0.1:6601/(Monitoring), 点击Management面板, 在Kibana选项选择Index Patterns 进行Create index pattern‘; 4) 在Index Patterns输入框输入索引的正则表达式(如不明白, 可以输入logstash*进行匹配, 正确匹配后下方会有绿色提示Success),如下图所示. 5) 点击Next Step进行下一步操作, 在Time Filter field name下拉框中选择@timestamp,点击Create index pattern; 6) 点击Discover面板进行查询, 即可显示在Production中运行kibana的Console面板产生的查询语句,如下图所示: 7) 选择Available Fields下方你想要的添加的字段,即可在右方显示对应的日志信息.如:我们选择path,query_body,responsetime字段,界面的显示效果如下所示: 8) 制作dashboard: (略)","categories":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://lylgjiavg.github.io/categories/Elastic-Stack/"}],"tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://lylgjiavg.github.io/tags/ElasticSearch/"},{"name":"Kibana","slug":"Kibana","permalink":"https://lylgjiavg.github.io/tags/Kibana/"},{"name":"Beats","slug":"Beats","permalink":"https://lylgjiavg.github.io/tags/Beats/"},{"name":"Logstash","slug":"Logstash","permalink":"https://lylgjiavg.github.io/tags/Logstash/"}]},{"title":"Logstash入门","slug":"3.Logstash入门","date":"2020-04-03T14:28:50.000Z","updated":"2020-04-04T16:59:03.059Z","comments":true,"path":"2020/04/03/3.Logstash入门/","link":"","permalink":"https://lylgjiavg.github.io/2020/04/03/3.Logstash入门/","excerpt":"","text":"Logstash入门Logstash简介 集中、转换和存储数据 Logstash 是开源的服务器端数据处理管道，能够同时从多个来源采集数据，转换数据，然后将数据发送到您最喜欢的“存储库”中。 Data Shipper (数据传送者) ETL Extract (采集数据) Transform (转换数据) Load (传输数据) Logstash处理流程 Input # 输入,其中输入源如下 file Redis beats kafka …… Filter # 处理,处理方式如下 grok mutate drop date Output # 输出,输出源如下 stdout Elasticsearch …… Input 和 Output配置例: # 输入配置 input {file { path =&gt; &quot;/tmp/abc.log&quot;}} # 输出配置 output { stdout {codec =&gt; rubydebug}}Filter配置 Grok 基于正则表达式提供了丰富可重用的模式(pattern) 基于此可以将非结构化数据作为结构化处理 Date 将字符串类型的时间字段转换为时间戳类型, 方便后续数据处理 Mutate 进行增加, 修改, 删除, 替换等字段相关的处理 ……. 例: # 待转换的数据 # 55.3.244.1 GET /index.html 15824 0.043 # 转换配置 %{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration} # 转换后的数据 { &quot;client&quot;: &quot;55.3.244.1&quot;, &quot;method&quot;: &quot;GET&quot;, &quot;request&quot;: &quot;/index.html&quot;, &quot;bytes&quot;: &quot;15824&quot;, &quot;duration&quot;: &quot;0.043&quot; }收集nginx log 下载Logstash 下载地址: https://www.elastic.co/cn 运行Logstash 解压 [jlc@localhost es]$ tar -zxf logstash-6.1.1.tar.gz Logstash配置文件 # Logstash从标准输入读取数据 input { stdin { } } # 数据处理流程 filter { # 将非结构化数据作为结构化处理 grok { match =&gt; { &quot;message&quot; =&gt; &#39;%{IPORHOST:remote_ip} - %{DATA:user_name} \\[%{HTTPDATE:time}\\] &quot;%{WORD:request_action} %{DATA:request} HTTP/%{NUMBER:http_version}&quot; %{NUMBER:response} %{NUMBER:bytes} &quot;%{DATA:referrer}&quot; &quot;%{DATA:agent}&quot;&#39; } } # 将时间字符串进行处理,转换为此条信息的时间戳 date { match =&gt; [ &quot;time&quot;, &quot;dd/MMM/YYYY:HH:mm:ss Z&quot; ] locale =&gt; en } # 将IP信息转换为地理位置信息 geoip { source =&gt; &quot;remote_ip&quot; target =&gt; &quot;geoip&quot; } # useragent处理 useragent { source =&gt; &quot;agent&quot; target =&gt; &quot;user_agent&quot; } } # 数据输出到标准输出,并按rubydebug进行解码(格式化) output { stdout { codec =&gt; rubydebug } }处理数据示例 # [jlc@localhost logstash-6.1.1]$ head -n 2 ../access.log 127.0.0.1 - - [27/Jan/2020:23:15:54 +0800] \"GET / HTTP/1.1\" 200 612 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\" 127.0.0.1 - - [27/Jan/2020:23:38:36 +0800] \"GET / HTTP/1.1\" 304 0 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\" 运行Logstash [jlc@localhost logstash-6.1.1]$ head -n 2 ../access.log | bin/logstash -f nginx_logstash.conf 标准输出数据 Sending Logstash's logs to /usr/local/es/logstash-6.1.1/logs which is now configured via log4j2.properties [2020-04-03T01:09:01,830][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>\"fb_apache\", :directory=>\"/usr/local/es/logstash-6.1.1/modules/fb_apache/configuration\"} [2020-04-03T01:09:01,868][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>\"netflow\", :directory=>\"/usr/local/es/logstash-6.1.1/modules/netflow/configuration\"} [2020-04-03T01:09:02,084][INFO ][logstash.setting.writabledirectory] Creating directory {:setting=>\"path.queue\", :path=>\"/usr/local/es/logstash-6.1.1/data/queue\"} [2020-04-03T01:09:02,091][INFO ][logstash.setting.writabledirectory] Creating directory {:setting=>\"path.dead_letter_queue\", :path=>\"/usr/local/es/logstash-6.1.1/data/dead_letter_queue\"} [2020-04-03T01:09:02,797][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified [2020-04-03T01:09:02,968][INFO ][logstash.agent ] No persistent UUID file found. Generating new UUID {:uuid=>\"5de9619e-a0ef-4ba6-a53d-0a966ee65eb9\", :path=>\"/usr/local/es/logstash-6.1.1/data/uuid\"} [2020-04-03T01:09:04,137][INFO ][logstash.runner ] Starting Logstash {\"logstash.version\"=>\"6.1.1\"} [2020-04-03T01:09:05,219][INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=>9600} [2020-04-03T01:09:10,505][INFO ][logstash.filters.geoip ] Using geoip database {:path=>\"/usr/local/es/logstash-6.1.1/vendor/bundle/jruby/2.3.0/gems/logstash-filter-geoip-5.0.2-java/vendor/GeoLite2-City.mmdb\"} [2020-04-03T01:09:10,991][INFO ][logstash.pipeline ] Starting pipeline {:pipeline_id=>\"main\", \"pipeline.workers\"=>2, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>5, \"pipeline.max_inflight\"=>250, :thread=>\"#\"} [2020-04-03T01:09:11,143][INFO ][logstash.pipeline ] Pipeline started {\"pipeline.id\"=>\"main\"} [2020-04-03T01:09:11,419][INFO ][logstash.agent ] Pipelines running {:count=>1, :pipelines=>[\"main\"]} { \"geoip\" => {}, \"request\" => \"/\", \"http_version\" => \"1.1\", \"@version\" => \"1\", \"bytes\" => \"612\", \"tags\" => [ [0] \"_geoip_lookup_failure\" ], \"user_agent\" => { \"os\" => \"Windows 10\", \"device\" => \"Other\", \"os_name\" => \"Windows 10\", \"build\" => \"\", \"minor\" => \"0\", \"name\" => \"Chrome\", \"major\" => \"78\", \"patch\" => \"3904\" }, \"message\" => \"127.0.0.1 - - [27/Jan/2020:23:15:54 +0800] \\\"GET / HTTP/1.1\\\" 200 612 \\\"-\\\" \\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\\\"\\r\", \"request_action\" => \"GET\", \"time\" => \"27/Jan/2020:23:15:54 +0800\", \"@timestamp\" => 2020-01-27T15:15:54.000Z, \"agent\" => \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\", \"host\" => \"localhost.localdomain\", \"user_name\" => \"-\", \"response\" => \"200\", \"referrer\" => \"-\", \"remote_ip\" => \"127.0.0.1\" } { \"geoip\" => {}, \"request\" => \"/\", \"http_version\" => \"1.1\", \"@version\" => \"1\", \"bytes\" => \"0\", \"tags\" => [ [0] \"_geoip_lookup_failure\" ], \"user_agent\" => { \"os\" => \"Windows 10\", \"device\" => \"Other\", \"os_name\" => \"Windows 10\", \"build\" => \"\", \"minor\" => \"0\", \"name\" => \"Chrome\", \"major\" => \"78\", \"patch\" => \"3904\" }, \"message\" => \"127.0.0.1 - - [27/Jan/2020:23:38:36 +0800] \\\"GET / HTTP/1.1\\\" 304 0 \\\"-\\\" \\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\\\"\\r\", \"request_action\" => \"GET\", \"time\" => \"27/Jan/2020:23:38:36 +0800\", \"@timestamp\" => 2020-01-27T15:38:36.000Z, \"agent\" => \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\", \"host\" => \"localhost.localdomain\", \"user_name\" => \"-\", \"response\" => \"304\", \"referrer\" => \"-\", \"remote_ip\" => \"127.0.0.1\" } [2020-04-03T01:09:12,993][INFO ][logstash.pipeline ] Pipeline terminated {\"pipeline.id\"=>\"main\"} 注: 其中_geoip_lookup_failure 此错误是由于,本机IP地址为私网IP, 无法转换为地理位置。","categories":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://lylgjiavg.github.io/categories/Elastic-Stack/"}],"tags":[{"name":"Logstash","slug":"Logstash","permalink":"https://lylgjiavg.github.io/tags/Logstash/"}]},{"title":"Beats入门","slug":"2.Beats、Filebeat入门","date":"2020-04-02T14:28:50.000Z","updated":"2020-04-05T18:40:14.678Z","comments":true,"path":"2020/04/02/2.Beats、Filebeat入门/","link":"","permalink":"https://lylgjiavg.github.io/2020/04/02/2.Beats、Filebeat入门/","excerpt":"","text":"Beats入门简介官方定义 Lightweight Data Shipper (轻量级数据传送者) Filebeat # 日志文件 Metricbeat # 度量数据 Packetbeat # 网络数据 Winlogbeat # Windows数据 Heartbeat # 健康检查 Filebeat轻量型日志采集器 处理流程: 输入Input 处理Filter 输出Output Prospector(勘探者): 负责监控日志文件信息 Harvester(收割者) :负责把Prospector监控到变化的日志信息发送到Spooler进行处理 知识点: 一个Filebeat可以有多个Prospector 每个日志文件拥有自己的Harvester Filebeat Input配置简介 yaml语法 input_type # 拥有两种类型 log # 指定本Prospector监控的是log日志文件 stdin # 指定本Prospector监控的是标准输入 例: # prospectors 为一个数组,Filebeat可以配置多个prospectors filebeat.prospectors: - input_type: log # 第一个prospector paths: - c:\\programdata\\elasticsearch\\logs\\* - input_type: stdin # 第二个prospector Filebeat Output配置简介 Console Elasticsearch Logstash Kafka Redis File 例1: # 输出信息到elasticsearch output.elasticsearch: hosts: [\"localhost:9200\"] username: \"elastic\" password: \"changeme\" 例2: # 输出信息到控制台 output.console: pretty: true # 表示把输出的json格式化,更方便阅读 Filebeat Filter配置简介 Input时处理 include_lines # 满足条件读取这一行 exclude_lines # 满足条件不读取这一行 exclude_files # 满足条件不读取这个文件 Output前处理 (– Processor) drop_event # 满足条件直接删除 drop_fields # 满足条件直接删除这个字段 decode_json_fields # 对满足条件的json字段进行解析 include_fields # 满足条件加入这些字段, 或者过滤通过这些字段 例1: processor: - drop_event: # 当正则匹配就删除这个信息 when: regexp: message: \"^DBG:\" 例2: processor: - decode_json_fields: fields: [\"inner\"] 例2用来处理形如下信息的信息 {&quot;outer&quot;: &quot;value&quot;, &quot;inner&quot;: &quot;{\\&quot;data\\&quot;: \\&quot;value\\&quot;}&quot;} 处理后结果为 {&quot;outer&quot;: &quot;value&quot;, &quot;inner&quot;: &quot;{&quot;data&quot;: &quot;value&quot;}&quot;} Filebeat + Elasticsearch Ingest Node Filebeat 缺乏数据转换的能力 Elasticsearch Ingest Node 新增的node类型 在数据写入es之前对数据进行处理转换 pipeline api (Ingest Node 使用的处理api) Filebeat Module简介对于社区常见的需求进行配置封装增加易用性,其中封装了如下常见需求: nginx Apache MySQL …… 封装内容: filebeat.yml配置 ingest node pipeline配置 Kibana dashboard 这些Module是最佳实践参考, 可以不使用,但其中蕴含的最佳配置是参考的不二人选. Filebeat 收集 nginx log 通过stdin收集日志 通过console输出结果 下载Filebeat ​ 下载地址: https://www.elastic.co/cn/ 运行filebeat命令 其中, 要收集的日志信息形如下: [jlc@localhost es]$ head -n 2 ./access.log 127.0.0.1 - - [27/Jan/2020:23:15:54 +0800] \"GET / HTTP/1.1\" 200 612 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\" 127.0.0.1 - - [27/Jan/2020:23:38:36 +0800] \"GET / HTTP/1.1\" 304 0 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\" 其中,使用的Filebeat配置文件(nginx.yml)内容如下: filebeat.prospectors: - type: stdin output.console: pretty: true 解压运行filebeat # 解压filebeat [jlc@localhost es]$ tar -zxf filebeat-6.1.1-linux-x86_64.tar.gz # 运行 [jlc@localhost filebeat-6.1.1-linux-x86_64]$ head -n 2 ../access.log | ./filebeat -e -c ./nginx.yml 运行输出结果 [jlc@localhost filebeat-6.1.1-linux-x86_64]$ head -n 2 ../access.log | ./filebeat -e -c ./nginx.yml 2020/04/01 16:55:38.579816 beat.go:436: INFO Home path: [/usr/local/es/filebeat-6.1.1-linux-x86_64] Config path: [/usr/local/es/filebeat-6.1.1-linux-x86_64] Data path: [/usr/local/es/filebeat-6.1.1-linux-x86_64/data] Logs path: [/usr/local/es/filebeat-6.1.1-linux-x86_64/logs] 2020/04/01 16:55:38.580153 beat.go:443: INFO Beat UUID: 150db773-661f-4fab-b139-07e0ffc8e768 2020/04/01 16:55:38.580182 beat.go:203: INFO Setup Beat: filebeat; Version: 6.1.1 2020/04/01 16:55:38.580475 metrics.go:23: INFO Metrics logging every 30s 2020/04/01 16:55:38.580689 module.go:76: INFO Beat name: localhost.localdomain 2020/04/01 16:55:38.583042 beat.go:276: INFO filebeat start running. 2020/04/01 16:55:38.583207 registrar.go:71: INFO No registry file found under: /usr/local/es/filebeat-6.1.1-linux-x86_64/data/registry. Creating a new registry file. 2020/04/01 16:55:38.586759 registrar.go:108: INFO Loading registrar data from /usr/local/es/filebeat-6.1.1-linux-x86_64/data/registry 2020/04/01 16:55:38.586816 registrar.go:119: INFO States Loaded from registrar: 0 2020/04/01 16:55:38.586869 filebeat.go:261: WARN Filebeat is unable to load the Ingest Node pipelines for the configured modules because the Elasticsearch output is not configured/enabled. If you have already loaded the Ingest Node pipelines or are using Logstash pipelines, you can ignore this warning. 2020/04/01 16:55:38.586889 crawler.go:48: INFO Loading Prospectors: 1 2020/04/01 16:55:38.587132 prospector.go:87: INFO Starting prospector of type: stdin; ID: 16876905907669988323 2020/04/01 16:55:38.587146 crawler.go:82: INFO Loading and starting Prospectors completed. Enabled prospectors: 1 2020/04/01 16:55:38.587232 registrar.go:150: INFO Starting Registrar 2020/04/01 16:55:38.587425 harvester.go:215: INFO Harvester started for file: - 2020/04/01 16:55:38.587716 harvester.go:238: INFO End of file reached: . Closing because close_eof is enabled. { \"@timestamp\": \"2020-04-01T16:55:38.587Z\", \"@metadata\": { \"beat\": \"filebeat\", \"type\": \"doc\", \"version\": \"6.1.1\" }, \"offset\": 190, \"message\": \"127.0.0.1 - - [27/Jan/2020:23:15:54 +0800] \\\"GET / HTTP/1.1\\\" 200 612 \\\"-\\\" \\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\\\"\", \"source\": \"\", \"prospector\": { \"type\": \"stdin\" }, \"beat\": { \"name\": \"localhost.localdomain\", \"hostname\": \"localhost.localdomain\", \"version\": \"6.1.1\" } } { \"@timestamp\": \"2020-04-01T16:55:38.587Z\", \"@metadata\": { \"beat\": \"filebeat\", \"type\": \"doc\", \"version\": \"6.1.1\" }, \"source\": \"\", \"offset\": 188, \"message\": \"127.0.0.1 - - [27/Jan/2020:23:38:36 +0800] \\\"GET / HTTP/1.1\\\" 304 0 \\\"-\\\" \\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\\\"\", \"prospector\": { \"type\": \"stdin\" }, \"beat\": { \"name\": \"localhost.localdomain\", \"hostname\": \"localhost.localdomain\", \"version\": \"6.1.1\" } } 2020/04/01 16:56:08.582776 metrics.go:39: INFO Non-zero metrics in the last 30s: beat.info.uptime.ms=30004 beat.memstats.gc_next=4473924 beat.memstats.memory_alloc=2924304 beat.memstats.memory_total=2924304 filebeat.events.active=2 filebeat.events.added=2 filebeat.harvester.closed=1 filebeat.harvester.open_files=-1 filebeat.harvester.running=0 filebeat.harvester.started=1 libbeat.config.module.running=0 libbeat.output.events.acked=2 libbeat.output.events.batches=1 libbeat.output.events.total=2 libbeat.output.type=console libbeat.output.write.bytes=1078 libbeat.pipeline.clients=0 libbeat.pipeline.events.active=0 libbeat.pipeline.events.published=2 libbeat.pipeline.events.total=2 libbeat.pipeline.queue.acked=2 registrar.states.current=0 registrar.writes=1 Packetbeat 简介 实时抓取网络包 自动解析应用层协议 ICMP (v4 and v6) DNS HTTP Mysql Redis …… Wireshark (Packetbeat 可以当做轻量级的Wireshark) Packetbeat解析HTTP协议解析Elasticsearch http请求 下载Packetbeat 下载地址: https://www.elastic.co/cn/ 配置配置文件(http.yml) packetbeat.interfaces.device: lo # packetbeat监听的网卡 packetbeat.protocols: # packetbeat监听的协议 - type: http ports: [9200] # packetbeat监听的端口 send_request: true include_body_for: [\"application/json\",\"x-www-form-urlencoded\"] output.console: pretty: true 运行Packetbeat # sudo 抓包时,需要有root权限 # -strict.perms=false Packetbeat运行时需要检查配置文件,加上这句就不会检查了,方便运行 [jlc@localhost packetbeat-6.1.1-linux-x86_64]$ sudo ./packetbeat -e -c http.yml -strict.perms=false 在浏览器输入请求 http://localhost:9200/ 控制台得到输出 { \"@timestamp\": \"2020-04-01T17:40:05.307Z\", \"@metadata\": { \"beat\": \"packetbeat\", \"type\": \"doc\", \"version\": \"6.1.1\" }, \"status\": \"OK\", \"proc\": \"\", \"path\": \"/\", \"query\": \"GET /\", \"beat\": { \"name\": \"localhost.localdomain\", \"hostname\": \"localhost.localdomain\", \"version\": \"6.1.1\" }, \"client_port\": 54914, \"port\": 9200, \"server\": \"localhost.localdomain\", \"type\": \"http\", \"http\": { \"request\": { \"headers\": { \"content-length\": 0 }, \"params\": \"\" }, \"response\": { \"code\": 200, \"phrase\": \"OK\", \"headers\": { \"content-length\": 278, \"content-type\": \"application/json; charset=UTF-8\" }, \"body\": \"\\u001f\\ufffd\\u0008\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000|\\ufffd[K\\ufffd@\\u0010\\ufffd\\ufffd\\ufffd+\\ufffd\\u003c\\ufffd!\\ufffd\\ufffd\\ufffd\\ufffdc\\ufffd\\ufffd\\ufffd ־ԗ\\ufffdI\\u0026fd/e/\\ufffdP\\ufffd\\ufffd\\ufffdE\\ufffdA\\ufffdq\\ufffd\\ufffdo\\ufffd\\ufffd\\\\\\u0002BB\\ufffd%\\ufffd$#\\ufffd\\ufffd5\\ufffd\\ufffdv\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd[\\u0007\\u0026\\ufffdUAp밴\\ufffdM\\ufffdL=\\ufffdc5LX?ܭء\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd#\\ufffd\\ufffd\\ufffd\\ufffdh\\ufffd\\u0000cQ\\ufffd\\ufffdw\\ufffd\\ufffd~\\ufffd\\ufffd\\u0005\\ufffd\\u0001\\\\F4\\ufffd\\ufffd\\ufffd\\ufffd\\u0017\\u001eE\\ufffd7\\ufffd6\\ufffdVT+\\u0006i=U+\\ufffd\\ufffd\\ufffd,\\ufffd露9M\\u000f,\\ufffdX\\ufffd\\ufffdE\\ufffd$\\ufffd\\ufffdS\\ufffdU\\ufffdd\\u001b\\ufffdz\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd*\\ufffdKP\\ufffd\\ufffdd\\u000b\\ufffd.J\\ufffdCKT(\\ufffd\\ufffd\\ufffdh /\\ufffd\\u003cq\\ufffd\\u0005\\nt\\ufffd\\ufffdYD˿\\u000c\\ufffd\\n\\u003e\\ufffd\\ufffd\\ufffd\\u000e\\ufffdN\\ufffd\\u000e\\ufffdq\\ufffdM\\ufffd\\u001a\\ufffd:jO\\ufffdJ\\ufffdg\\ufffdֆ\\ufffd\\ufffd/\\u000f\\ufffd\\ufffd\\u0017\\u0000\\u0000\\u0000\\ufffd\\ufffd\\u0003\\u0000\\ufffd+Cɰ\\u0001\\u0000\\u0000\" } }, \"bytes_out\": 389, \"client_server\": \"localhost.localdomain\", \"client_ip\": \"::1\", \"bytes_in\": 385, \"client_proc\": \"\", \"responsetime\": 3, \"request\": \"GET / HTTP/1.1\\r\\nHost: localhost:9200\\r\\nUser-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0\\r\\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\\r\\nAccept-Language: zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2\\r\\nAccept-Encoding: gzip, deflate\\r\\nConnection: keep-alive\\r\\nUpgrade-Insecure-Requests: 1\\r\\nCache-Control: max-age=0\\r\\n\\r\\n\", \"ip\": \"::1\", \"method\": \"GET\" } { \"@timestamp\": \"2020-04-01T17:40:05.407Z\", \"@metadata\": { \"beat\": \"packetbeat\", \"type\": \"doc\", \"version\": \"6.1.1\" }, \"bytes_in\": 340, \"client_ip\": \"::1\", \"status\": \"OK\", \"proc\": \"\", \"client_port\": 54914, \"client_proc\": \"\", \"client_server\": \"localhost.localdomain\", \"query\": \"GET /favicon.ico\", \"method\": \"GET\", \"server\": \"localhost.localdomain\", \"request\": \"GET /favicon.ico HTTP/1.1\\r\\nHost: localhost:9200\\r\\nUser-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0\\r\\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\\r\\nAccept-Language: zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2\\r\\nAccept-Encoding: gzip, deflate\\r\\nConnection: keep-alive\\r\\n\\r\\n\", \"path\": \"/favicon.ico\", \"bytes_out\": 1652, \"port\": 9200, \"http\": { \"request\": { \"headers\": { \"content-length\": 0 }, \"params\": \"\" }, \"response\": { \"phrase\": \"OK\", \"headers\": { \"content-length\": 1559, \"content-type\": \"image/x-icon\" }, \"code\": 200 } }, \"ip\": \"::1\", \"beat\": { \"name\": \"localhost.localdomain\", \"hostname\": \"localhost.localdomain\", \"version\": \"6.1.1\" }, \"type\": \"http\", \"responsetime\": 16 } { \"@timestamp\": \"2020-04-01T17:39:59.949Z\", \"@metadata\": { \"beat\": \"packetbeat\", \"type\": \"doc\", \"version\": \"6.1.1\" }, \"client_proc\": \"\", \"client_ip\": \"127.0.0.1\", \"proc\": \"\", \"responsetime\": 7524, \"query\": \"GET /_nodes/_local\", \"bytes_out\": 519, \"server\": \"localhost.localdomain\", \"beat\": { \"name\": \"localhost.localdomain\", \"hostname\": \"localhost.localdomain\", \"version\": \"6.1.1\" }, \"request\": \"GET /_nodes/_local?filter_path=nodes.*.settings.tribe HTTP/1.1\\r\\nHost: localhost:9200\\r\\nContent-Length: 0\\r\\nConnection: keep-alive\\r\\n\\r\\n\", \"ip\": \"127.0.0.1\", \"path\": \"/_nodes/_local\", \"method\": \"GET\", \"client_port\": 48786, \"http\": { \"response\": { \"body\": \"HTTP/1.1 200 OK\\r\\ncontent-type: application/json; charset=UTF-8\\r\\ncontent-length: 329\\r\\n\\r\\n{\\\"nodes\\\":{\\\"Eyro1LLhQOSsyBQNJV86kQ\\\":{\\\"ip\\\":\\\"127.0.0.1\\\",\\\"version\\\":\\\"6.1.1\\\",\\\"http\\\":{\\\"publish_address\\\":\\\"127.0.0.1:9200\\\"}},\\\"1P40_XHRTKKnKP-covkxLg\\\":{\\\"ip\\\":\\\"127.0.0.1\\\",\\\"version\\\":\\\"6.1.1\\\",\\\"http\\\":{\\\"publish_address\\\":\\\"127.0.0.1:7200\\\"}},\\\"_d5BzFgWQZGxUcT9KAEFaA\\\":{\\\"ip\\\":\\\"127.0.0.1\\\",\\\"version\\\":\\\"6.1.1\\\",\\\"http\\\":{\\\"publish_address\\\":\\\"127.0.0.1:8200\\\"}}}}HTTP/1.1 200 OK\\r\", \"code\": 200, \"phrase\": \"OK\", \"headers\": { \"content-length\": 432, \"content-type\": \"application/json; charset=UTF-8\" } }, \"request\": { \"headers\": { \"content-length\": 0 }, \"params\": \"filter_path=nodes.%2A.settings.tribe\" } }, \"client_server\": \"localhost.localdomain\", \"port\": 9200, \"bytes_in\": 131, \"type\": \"http\", \"status\": \"OK\" }","categories":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://lylgjiavg.github.io/categories/Elastic-Stack/"}],"tags":[{"name":"Beats","slug":"Beats","permalink":"https://lylgjiavg.github.io/tags/Beats/"}]},{"title":"ElasticSearch与Kibana入门","slug":"1.ElasticSearch与Kibana","date":"2020-04-01T14:28:50.000Z","updated":"2020-04-05T18:39:51.978Z","comments":true,"path":"2020/04/01/1.ElasticSearch与Kibana/","link":"","permalink":"https://lylgjiavg.github.io/2020/04/01/1.ElasticSearch与Kibana/","excerpt":"","text":"ElasticSearch与Kibana入门Elasticsearch安装与运行 安装JDK1.8 下载安装Elasticsearch 运行 bin/elasticsearch Elasticsearch安装与运行详细步骤1.安装JDK1.8(略) 2.下载Elasticsearch下载地址: https://www.elastic.co/cn/ 3.运行 bin/elasticsearch 解压安装包 [jlc@localhost es]$ tar -zxf elasticsearch-6.1.1.tar.gz # 进入解压后的目录 [jlc@localhost es]$ cd elasticsearch-6.1.1/ 运行 bin/elasticsearch命令 [jlc@localhost elasticsearch-6.1.1]$ bin/elasticsearch 4.访问Elasticsearch在浏览器中输入 127.0.0.1:9200,页面将会输出以下信息,证明Elasticsearch运行成功. { \"name\" : \"Eyro1LL\", \"cluster_name\" : \"elasticsearch\", \"cluster_uuid\" : \"BH492TyERByKamIjYeaROQ\", \"version\" : { \"number\" : \"6.1.1\", \"build_hash\" : \"bd92e7f\", \"build_date\" : \"2017-12-17T20:23:25.338Z\", \"build_snapshot\" : false, \"lucene_version\" : \"7.1.0\", \"minimum_wire_compatibility_version\" : \"5.6.0\", \"minimum_index_compatibility_version\" : \"5.0.0\" }, \"tagline\" : \"You Know, for Search\" } Elasticsearch配置说明 配置文件位于config目录中 elasticsearch.yml # es的相关配置 jvm.options # jvm的相关参数 log4j2.properties # 日志相关配置 1.elasticsearch.yml关键配置说明 cluster.name # 集群名称,以此作为是否同一集群的判断条件 node.name # 节点名称,以此作为集群中不同节点的区分条件 network.host/http.port # 网络地址端口,用于http和transport服务使用 path.data # 数据存储地址 path.log # 日志存储地址 2.Development与Production模式说明 以transport的地址(network.host)是否绑定在localhost为判断标准 Development模式在启动时会以warning的方式提示配置检查异常 Production模式下会以error的方式提示配置检查异常并退出 3.参数修改的第二种方式[jlc@localhost elasticsearch-6.1.1]$ bin/elasticsearch -Ehttp.port=19200 Elasticsearch本地启动集群方式新建三个终端窗口,分别输入以下三个命令,便可创建一个节点数为3的Elasticsearch集群 [jlc@localhost elasticsearch-6.1.1]$ bin/elasticsearch [jlc@localhost elasticsearch-6.1.1]$ bin/elasticsearch -Ehttp.port=8200 -Epath.data=node2 [jlc@localhost elasticsearch-6.1.1]$ bin/elasticsearch -Ehttp.port=7200 -Epath.data=node3 在浏览器中输入127.0.0.1:9200/_cat/nodes便会输出Elasticsearch节点情况,如下所示 # 地址栏: 127.0.0.1:9200/_cat/nodes 127.0.0.1 14 97 7 0.62 0.38 0.21 mdi - 1P40_XH 127.0.0.1 12 97 3 0.62 0.38 0.21 mdi * Eyro1LL 127.0.0.1 14 97 12 0.62 0.38 0.21 mdi - _d5BzFg # 地址栏: 127.0.0.1:9200/_cat/nodes?v # 将会显示节点信息的标题栏 ip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name 127.0.0.1 14 97 3 0.46 0.39 0.22 mdi - 1P40_XH 127.0.0.1 12 97 3 0.46 0.39 0.22 mdi * Eyro1LL 127.0.0.1 14 97 3 0.46 0.39 0.22 mdi - _d5BzFg在浏览器中输入127.0.0.1:9200/_cluster/stats将会输出Elasticsearch集群详细信息,如下所示 { \"_nodes\": { \"total\": 3, \"successful\": 3, \"failed\": 0 }, \"cluster_name\": \"elasticsearch\", \"timestamp\": 1585667737502, \"status\": \"green\", \"indices\": { \"count\": 0, \"shards\": {}, \"docs\": { \"count\": 0, \"deleted\": 0 }, \"store\": { \"size_in_bytes\": 0 }, \"fielddata\": { \"memory_size_in_bytes\": 0, \"evictions\": 0 }, \"query_cache\": { \"memory_size_in_bytes\": 0, \"total_count\": 0, \"hit_count\": 0, \"miss_count\": 0, \"cache_size\": 0, \"cache_count\": 0, \"evictions\": 0 }, \"completion\": { \"size_in_bytes\": 0 }, \"segments\": { \"count\": 0, \"memory_in_bytes\": 0, \"terms_memory_in_bytes\": 0, \"stored_fields_memory_in_bytes\": 0, \"term_vectors_memory_in_bytes\": 0, \"norms_memory_in_bytes\": 0, \"points_memory_in_bytes\": 0, \"doc_values_memory_in_bytes\": 0, \"index_writer_memory_in_bytes\": 0, \"version_map_memory_in_bytes\": 0, \"fixed_bit_set_memory_in_bytes\": 0, \"max_unsafe_auto_id_timestamp\": -9223372036854776000, \"file_sizes\": {} } }, \"nodes\": { \"count\": { \"total\": 3, \"data\": 3, \"coordinating_only\": 0, \"master\": 3, \"ingest\": 3 }, \"versions\": [ \"6.1.1\" ], \"os\": { \"available_processors\": 6, \"allocated_processors\": 6, \"names\": [ { \"name\": \"Linux\", \"count\": 3 } ], \"mem\": { \"total_in_bytes\": 11862110208, \"free_in_bytes\": 346312704, \"used_in_bytes\": 11515797504, \"free_percent\": 3, \"used_percent\": 97 } }, \"process\": { \"cpu\": { \"percent\": 0 }, \"open_file_descriptors\": { \"min\": 191, \"max\": 192, \"avg\": 191 } }, \"jvm\": { \"max_uptime_in_millis\": 2551480, \"versions\": [ { \"version\": \"1.8.0_222-ea\", \"vm_name\": \"OpenJDK 64-Bit Server VM\", \"vm_version\": \"25.222-b03\", \"vm_vendor\": \"Oracle Corporation\", \"count\": 3 } ], \"mem\": { \"heap_used_in_bytes\": 499037568, \"heap_max_in_bytes\": 3168927744 }, \"threads\": 79 }, \"fs\": { \"total_in_bytes\": 27899465728, \"free_in_bytes\": 17078177792, \"available_in_bytes\": 17078177792 }, \"plugins\": [], \"network_types\": { \"transport_types\": { \"netty4\": 3 }, \"http_types\": { \"netty4\": 3 } } } } Kibana安装与演示Kibana安装与运行 下载安装Kibana 运行bin/kibana Kibana安装与运行详细步骤 下载Kibana 下载地址: https://www.elastic.co/cn/ 运行Kibana 解压安装包 [jlc@localhost es]$ tar -zxf kibana-6.1.1-linux-x86_64.tar.gz # 进入kibana目录 [jlc@localhost es]$ cd kibana-6.1.1-linux-x86_64/ 配置Kibana配置文件(config/kibana.yml),修改其对应的Elasticsearch运行地址 # 本讲解修改为本地的一个Elasticsearch的node节点 elasticsearch.url: \"http://localhost:9200\" 运行Kibana [jlc@localhost kibana-6.1.1-linux-x86_64]$ bin/kibana 访问Kibana 在浏览器中输入localhost:5601即可查看Kibana运行后的界面,如下图所示 Kibana配置说明 配置位于config文件夹中 kibana.yml关键配置说明 server.host/server.port # 访问kibana用的地址和端口 elasticsearch.url # 待访问Elasticsearch的地址 Kibana常用功能说明 Discover # 数据搜索查看 Visualize # 图表制作 Dashboard # 仪表盘制作 Timelion # 实时数据的高级可视化分析 DevTools # 开发者工具 Management # Kibana配置 Elasticsearch常用术语 Document # 文档数据(可看为) Index # 索引 Type # 索引中的数据类型 Field # 字段,文档的属性 Query DSL # 查询的语法 Elasticsearch CRUD Create # 创建文档 Read # 读取文档 Update # 更新文档 Delete # 删除文档 Elasticsearch Create在Kibana的Dev Tools项下输入如下语句 # 将会创建 Index为: account, Type为: person, Document的id为: 1 的文档 # 其中, 插入语句下的Json语句为待插入文档的内容 POST /account/person/1 { &quot;name&quot;: &quot;John&quot;, &quot;lastname&quot;: &quot;Doe&quot;, &quot;job_description&quot;: &quot;Systems adminostrator and linux specialit&quot; }其中,在Elasticsearch完成插入后,返回的信息如下所示 { \"_index\": \"account\", \"_type\": \"person\", \"_id\": \"1\", \"_version\": 1, \"result\": \"created\", \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 0, \"_primary_term\": 1 } Elasticsearch Read在Kibana的Dev Tools项下输入如下语句 # 将会获取 Index为: account, Type为: person, Document的id为: 1 的文档信息 GET /account/person/1其中,在Elasticsearch完成请求后,返回的信息如下所示 { \"_index\": \"account\", \"_type\": \"person\", \"_id\": \"1\", \"_version\": 1, \"found\": true, \"_source\": { \"name\": \"John\", \"lastname\": \"Doe\", \"job_description\": \"Systems adminostrator and linux specialit\" } } Elasticsearch Update在Kibana的Dev Tools项下输入如下语句 # 将会修改 Index为: account, Type为: person下的 Document的id为: 1 的文档信息 # 其中修改的内容为&quot;doc&quot;: {} 里的内容信息 POST /account/person/1/_update { &quot;doc&quot;: { &quot;job_description&quot;: &quot;Systems adminostrator and linux specialist&quot; } }其中,在Elasticsearch完成修改后,返回的信息如下所示 { \"_index\": \"account\", \"_type\": \"person\", \"_id\": \"1\", \"_version\": 2, \"result\": \"updated\", \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 1, \"_primary_term\": 1 } Elasticsearch Delete在Kibana的Dev Tools项下输入如下语句 # 将会删除 Index为: account, Type为: person下的 Document的id为: 1 的文档信息 DELETE /account/person/1其中,在Elasticsearch完成删除后,返回的信息如下所示 { \"_index\": \"account\", \"_type\": \"person\", \"_id\": \"1\", \"_version\": 3, \"result\": \"deleted\", \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 2, \"_primary_term\": 1 } Elasticsearch Query Query String 例: GET /account/person/_search?q=john在Elasticsearch完成查询后,返回的信息如下所示 { \"took\": 448, \"timed_out\": false, \"_shards\": { \"total\": 5, \"successful\": 5, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": 1, \"max_score\": 0.2876821, \"hits\": [ { \"_index\": \"account\", \"_type\": \"person\", \"_id\": \"1\", \"_score\": 0.2876821, \"_source\": { \"name\": \"John\", \"lastname\": \"Doe\", \"job_description\": \"Systems adminostrator and linux specialit\" } } ] } } Query DSL 例: GET /account/person/_search { &quot;query&quot;: { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;john&quot; } } } }在Elasticsearch完成查询后,返回的信息如下所示 { \"took\": 52, \"timed_out\": false, \"_shards\": { \"total\": 5, \"successful\": 5, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": 1, \"max_score\": 0.2876821, \"hits\": [ { \"_index\": \"account\", \"_type\": \"person\", \"_id\": \"1\", \"_score\": 0.2876821, \"_source\": { \"name\": \"John\", \"lastname\": \"Doe\", \"job_description\": \"Systems adminostrator and linux specialit\" } } ] } }","categories":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://lylgjiavg.github.io/categories/Elastic-Stack/"}],"tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://lylgjiavg.github.io/tags/ElasticSearch/"},{"name":"Kibana","slug":"Kibana","permalink":"https://lylgjiavg.github.io/tags/Kibana/"}]},{"title":"SpringBoot Cache初体验","slug":"SpringBoot缓存初体验","date":"2019-10-17T16:00:00.000Z","updated":"2019-10-18T15:56:54.156Z","comments":true,"path":"2019/10/18/SpringBoot缓存初体验/","link":"","permalink":"https://lylgjiavg.github.io/2019/10/18/SpringBoot缓存初体验/","excerpt":"","text":"SpringBoot Cache初体验一、JSR107 缓存规范​ Java Caching定义了5个核心接口，分别是CachingProvider, CacheManager, Cache, Entry 和 Expiry。 CachingProvider：定义了创建、配置、获取、管理和控制多个CacheManager。一个应用可以在运行期访问多个CachingProvider。 CacheManager：定义了创建、配置、获取、管理和控制多个唯一命名的Cache，这些Cache存在于CacheManager的上下文中。一个CacheManager仅被一个CachingProvider所拥有。 Cache：是一个类似Map的数据结构并临时存储以Key为索引的值。一个Cache仅被一个CacheManager所拥有。 Entry：是一个存储在Cache中的key-value对。 Expiry：每一个存储在Cache中的条目有一个定义的有效期。一旦超过这个时间，条目为过期的状态。一旦过期，条目将不可访问、更新和删除。缓存有效期可以通过ExpiryPolicy设置。 二、Spring缓存抽象Spring从3.1开始定义了org.springframework.cache.Cache和org.springframework.cache.CacheManager接口来统一不同的缓存技术；并支持使用JCache（JSR-107）注解简化我们开发； Cache接口为缓存的组件规范定义，包含缓存的各种操作集合； Cache接口下Spring提供了各种xxxCache的实现；如RedisCache，EhCacheCache , ConcurrentMapCache等； ​ ​ 每次调用需要缓存功能的方法时，Spring会检查指定参数的指定的目标方法是否已经被调用过；如果有就直接从缓存中获取方法调用后的结果，如果没有就调用方法并缓存结果后返回给用户。下次调用直接从缓存中获取。 ​ 使用Spring缓存抽象时我们需要关注以下两点： 确定方法需要被缓存以及他们的缓存策略 从缓存中读取之前缓存存储的数据 三、重要概念&amp;缓存注解 Cache 缓存接口，定义缓存操作。实现有：RedisCache、EhCacheCache、ConcurrentMapCache等 CacheManager 缓存管理器，管理各种缓存（Cache）组件 @Cacheable 主要针对方法配置，能够根据方法的请求参数对其结果进行缓存 @CacheEvict 清空缓存 @CachePut 保证方法被调用，又希望结果被缓存。 @EnableCaching 开启基于注解的缓存 keyGenerator 缓存数据时key生成策略 serialize 缓存数据时value序列化策略 @Cacheable/@CachePut/@CacheEvict主要的参数 Cache SpEL available metadata 四、缓存使用 引入spring-boot-starter-cache模块 &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-cache&lt;/artifactId> &lt;/dependency> @EnableCaching开启缓存 @MapperScan(\"club.lylgjiang.cache.mapper\") @EnableCaching @SpringBootApplication public class SpringBootAdvancedApplication { public static void main(String[] args) { SpringApplication.run(SpringBootAdvancedApplication.class, args); } } 使用缓存注解 // Mapper层 public interface EmployeeMapper { @Select(\"SELECT * FROM employee WHERE id=#{id}\") public Employee selectEmpById(Integer id); } // Service层 @Service public class EmployeeService { @Autowired EmployeeMapper employeeMapper; // 添加缓存注解 @Cacheable(\"employee\") public Employee getEmpById(Integer id) { return employeeMapper.selectEmpById(id); } } // Controller层 @RestController public class EmpController { @Autowired EmployeeService employeeService; @GetMapping(\"/emp/{id}\") public Employee showEmployee(@PathVariable Integer id) { return employeeService.getEmpById(id); } } 其他设置： // 设置项目端口为80 server.port=80 // 设置springboot的club.lylgjiang.cache(mapper层在此包中)包下日志级别为debug logging.level.club.lylgjiang.cache=debug 验证： 在浏览器中输入http://localhost/emp/3 其中浏览器显示数据与控制台输出数据如下图所示： 刷新浏览器（F5）：控制台无变化，证明无响应的SQL语句执行，缓存成功。","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://lylgjiavg.github.io/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://lylgjiavg.github.io/tags/SpringBoot/"},{"name":"Cache","slug":"Cache","permalink":"https://lylgjiavg.github.io/tags/Cache/"}]},{"title":"RabbitMQ-死信队列","slug":"RabbitMQ12","date":"2019-09-10T07:20:02.000Z","updated":"2019-09-10T08:10:31.962Z","comments":true,"path":"2019/09/10/RabbitMQ12/","link":"","permalink":"https://lylgjiavg.github.io/2019/09/10/RabbitMQ12/","excerpt":"","text":"死信队列 利用DLX,当消息在一个队列中变成死信(dead message)之后,他能够被重新publish到另一个Exchange,这个Exchange就是DLX. 消息变成死信有以下几种情况 消息被拒绝(basic.reject/basic.nack)并且requeue=false 消息TTL过期 队列到达最大长度 死信队列特点 DLX也是一个正常的Exchange,和一般的Exchange没有区别,它能在任何的队列上被指定,实际上就是设置某个队列的属性. 当这个队列中有死信时,RabbitMQ就会自动将这个消息重新发布到设置的这个Exchange上去,进而被路由到另一个队列. 可以监听这个队列中消息做相应的处理,这个特性可以弥补RabbitMQ3.0以前支持的immediate参数的功能. 死信队列设置 首先需要设置死信队列的exchange和queue,然后进行绑定;例: Exchange:dlx.exchange Queue:dlx.queue RoutingKey:# 然后我们进行正常声明交换机、队列、绑定，只不过我们需要在队列上加一个参数即可:arguments.put(“x-dead-letter-exchange”, “dlx.exchange”); 这样消息在过期、requeue、队列在到达最大长度时，消息就可以直接路由到死信队列！ 死信队列实现 import com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; import lylgjiavg.step02.limit.MyConsumer; import java.util.HashMap; import java.util.Map; /** * @Classname Consumer * @Description 死信队列：消费端 * @Date 2019/9/10 15:48 * @Created by Jiavg */ public class Consumer { public static void main(String[] args) throws Exception{ ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(\"127.0.0.1\"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost(\"/\"); connectionFactory.setUsername(\"guest\"); connectionFactory.setPassword(\"guest\"); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); String exchangeName = \"test_DLX_Exchange\"; String routingKey = \"DLX.save\"; String queueName = \"test_DLX_Queue\"; channel.exchangeDeclare(exchangeName, \"topic\", true, false, null); Map&lt;String, Object> arguments = new HashMap&lt;>(); arguments.put(\"x-dead-letter-exchange\", \"dlx.exchange\"); // 这个arguments属性要设置到声明队列上 channel.queueDeclare(queueName, true,false,false, arguments); channel.queueBind(queueName, exchangeName, routingKey); // 要进行死信队列的声明 channel.exchangeDeclare(\"dlx.exchange\", \"topic\", true, false, null); channel.queueDeclare(\"dlx.queue\", true, false, false, null); channel.queueBind(\"dlx.queue\", \"dlx.exchange\", \"#\"); channel.basicConsume(queueName, true, new MyConsumer(channel)); } }","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/categories/消息中间件/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://lylgjiavg.github.io/tags/RabbitMQ/"},{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/tags/消息中间件/"}]},{"title":"RabbitMQ-TTL队列/消息","slug":"RabbitMQ11","date":"2019-09-10T06:26:07.000Z","updated":"2019-09-10T07:21:25.856Z","comments":true,"path":"2019/09/10/RabbitMQ11/","link":"","permalink":"https://lylgjiavg.github.io/2019/09/10/RabbitMQ11/","excerpt":"","text":"RabbitMQ之TTL队列/消息 TTL是Time To Live的缩写,也就是生存时间. RabbitMQ支持消息的过期时间,在消息发送时可以进行制定. RabbitMQ支持队列的过期时间,从消息入队列开始计算,只要超过了队列的超时时间配置,那么消息会自动的清除. TTL队列 在设置队列的超时时间有两种方式,一是:通过Java代码声明队列时设置其超时时间设置;二是:通过RabbitMQ管控台的可视化操作下声明队列并进行设置其超时时间参数配置. TTL队列声明方式一:声明队列并设置其TTL import com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; import com.rabbitmq.client.QueueingConsumer; import java.util.HashMap; import java.util.Map; /** * @Classname Consumer * @Description TTL队列/消息：消费者 * @Date 2019/9/10 14:36 * @Created by Jiavg */ public class Consumer { public static void main(String[] args) throws Exception{ ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(\"127.0.0.1\"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost(\"/\"); connectionFactory.setUsername(\"guest\"); connectionFactory.setPassword(\"guest\"); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); String exchangeName = \"test_TTL_Exchange\"; String routingKey = \"TTL.#\"; String queueName = \"test_TTL_Queue\"; // 创建声明队列所用到的参数集合,并设置其超时时间x-message-ttl为10000(10秒) Map&lt;String, Object> arguments = new HashMap&lt;>(); arguments.put(\"x-message-ttl\", 10000); // 声明交换机、声明队列、绑定队列到交换机 channel.exchangeDeclare(exchangeName, \"topic\", true, false, null); channel.queueDeclare(queueName, true, false, false, arguments); channel.queueBind(queueName, exchangeName, routingKey); QueueingConsumer queueingConsumer = new QueueingConsumer(channel); channel.basicConsume(queueName, true, queueingConsumer); while (true) { QueueingConsumer.Delivery delivery = queueingConsumer.nextDelivery(); byte[] body = delivery.getBody(); System.out.printf(\"消费端:\" + new String(body)); } } } 管控台变化-超时队列信息超时队列相关信息 验证超时队列在管控台发送一条消息 信息处理过程 由上图可知,超时队列test_TTL_Queue中的消息在经过指定时间后会自动清除 TTL队列声明方式二:声明队列并设置其TTL TTL消息 在设置消息的超时时间有两种方式,一是:通过Java代码发送消息时设置其超时时间设置;二是:通过RabbitMQ管控台的可视化操作下发送消息并进行设置其超时时间参数配置(了解). TTL消息声明方式一:声明消息并设置其TTL import com.rabbitmq.client.*; import java.io.IOException; /** * @Classname Producer * @Description TTL队列/消息：生产者 * @Date 2019/9/10 14:38 * @Created by Jiavg */ public class Producer { public static void main(String[] args) throws Exception{ ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(\"127.0.0.1\"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost(\"/\"); connectionFactory.setUsername(\"guest\"); connectionFactory.setPassword(\"guest\"); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); String exchangeName = \"test_Topic_Exchange\"; String routingKey = \"test.ttl\"; String msg = \"Hello World send TTL Message\"; AMQP.BasicProperties properties = new AMQP.BasicProperties.Builder() .deliveryMode(2) .contentEncoding(\"UTF-8\") // 设置消息的超时时间 .expiration(\"10000\") .build(); channel.basicPublish(exchangeName, routingKey, properties, msg.getBytes()); } } 验证超时消息信息处理过程 由上图可知,设置了超时时间的消息在经过指定时间后会被自动清除. TTL消息声明方式二:声明消息并设置其TTL","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/categories/消息中间件/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://lylgjiavg.github.io/tags/RabbitMQ/"},{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/tags/消息中间件/"}]},{"title":"RabbitMQ-消费端ACK与重回队列","slug":"RabbitMQ10","date":"2019-09-09T12:27:02.000Z","updated":"2019-09-09T13:52:13.645Z","comments":true,"path":"2019/09/09/RabbitMQ10/","link":"","permalink":"https://lylgjiavg.github.io/2019/09/09/RabbitMQ10/","excerpt":"","text":"RabbitMQ的消费端ACK与重回队列消费端的手工ACK和NACK 消费端进行消费的时候,如果由于业务异常我们可以进行日志的记录,然后进行补偿. 如果由于服务器宕机等严重问题,那么我们就需要手工进行ACK保障消费端消费成功. 消费端的重回队列 消费端重回队列是为了对没有处理成功的消息,把消息重新传递给Broker. 一般我们在实际应用中,都会关闭重回队列,也就是设置为false. 消费端ACK与重回队列代码演示消费端自定义监听 import com.rabbitmq.client.AMQP; import com.rabbitmq.client.Channel; import com.rabbitmq.client.DefaultConsumer; import com.rabbitmq.client.Envelope; import java.io.IOException; /** * @Classname MyConsumer * @Description 消费端ACK:自定义客户端监听 * @Date 2019/9/8 13:15 * @Created by Jiavg */ public class MyConsumer extends DefaultConsumer { private Channel channel; /** * Constructs a new instance and records its association to the passed-in channel. * @param channel the channel to which this consumer is attached */ public MyConsumer(Channel channel) { super(channel); this.channel = channel; } @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { System.out.println(\"body:\" + new String(body)); // 延时ack try { Thread.sleep(10000); } catch (InterruptedException e) { e.printStackTrace(); } if((Integer)properties.getHeaders().get(\"num\") == 1){ // basicNack(long deliveryTag, boolean multiple, boolean requeue); channel.basicNack(envelope.getDeliveryTag(), false, true); }else { // basicAck(long deliveryTag, boolean multiple) throws IOException; channel.basicAck(envelope.getDeliveryTag(), false); } } } 注意: 在自定义消费端监听处理消息的handleDelivery()方法中,我们使用Thread.sleep(10000);延时10以方便我们能够详细的看到消息的消费状况. 在处理消息中,我们通过判断消息的properties中的headers配置参数中的num来决定此条消息是进行Ack确认还是进行NAck进行重回队列(requeue设置为true). 消费端实现 import com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; /** * @Classname Consumer * @Description 消费端ACK：消费端 * @Date 2019/9/8 10:48 * @Created by Jiavg */ public class Consumer { public static void main(String[] args) throws Exception{ ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(\"127.0.0.1\"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost(\"/\"); connectionFactory.setUsername(\"guest\"); connectionFactory.setPassword(\"guest\"); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); String exchangeName = \"test_Ack_Exchange\"; String routingKey = \"Ack.#\"; String queueName = \"test_Ack_Queue\"; // 声明交换机、声明队列、绑定队列到交换机 channel.exchangeDeclare(exchangeName, \"topic\", true, false, null); channel.queueDeclare(queueName, true, false, false, null); channel.queueBind(queueName, exchangeName, routingKey); // autoAck设置为false channel.basicConsume(queueName, false, new MyConsumer(channel)); } } 消费端运行-管控台变化交换机变化 交换机绑定情况 生产端实现 import com.rabbitmq.client.AMQP; import com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; import java.util.HashMap; import java.util.Map; /** * @Classname Producer * @Description 消费端ACK：生产者 * @Date 2019/9/8 10:50 * @Created by Jiavg */ public class Producer { public static void main(String[] args) throws Exception{ ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(\"127.0.0.1\"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost(\"/\"); connectionFactory.setUsername(\"guest\"); connectionFactory.setPassword(\"guest\"); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); String exchangeName = \"test_Ack_Exchange\"; String routingKey = \"Ack.save\"; for (int i = 0; i &lt; 4; i++){ String msg = \"Hello World send Ack Message:\" + i; Map&lt;String, Object> headers = new HashMap&lt;>(); headers.put(\"num\", i); AMQP.BasicProperties properties = new AMQP.BasicProperties.Builder() .deliveryMode(2) .contentEncoding(\"UTF-8\") .headers(headers) .build(); channel.basicPublish(exchangeName, routingKey,true, properties, msg.getBytes()); } } } 注意:投递模式deliveryMode设置为2,为就是将消息设置为持久化的，此时rabbitmq就会将消息持久化到磁盘上去(要使此模式生效,相应队列也应设置为持久化). 生产端运行-管控台和控制台变化管控台变化 控制台变化 由图结合自定义消费端监听可知,在发送的四条消息中,有一条消息被消费端NAck,故在队列中一直有一条消息未被ack.并且此条消息由于requeue设置为true,故此条消息被反复重回给队列并被此队列反复投递给此消费端(由于此队列只绑定了一个消费端).","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/categories/消息中间件/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://lylgjiavg.github.io/tags/RabbitMQ/"},{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/tags/消息中间件/"}]},{"title":"RabbitMQ-消费端自定义监听","slug":"RabbitMQ08-1","date":"2019-09-09T07:54:16.000Z","updated":"2019-09-09T08:40:02.826Z","comments":true,"path":"2019/09/09/RabbitMQ08-1/","link":"","permalink":"https://lylgjiavg.github.io/2019/09/09/RabbitMQ08-1/","excerpt":"","text":"消费端自定义监听 我们一般就是在代码中编写while循环,进行consumer.nextDelivery()方法进行获取下一条消息,然后进行消费处理. 但是我们使用自定义的Consumer更加的方便,解耦性更加的强,也是在实际工作中最常用的使用方式! 实现消费端自定义监听的两种方式实现com.rabbitmq.client.Consumer接口 import java.io.IOException; /** * Interface for application callback objects to receive notifications and messages from * a queue by subscription. * Most implementations will subclass {@link DefaultConsumer}. * &lt;p/> * The methods of this interface are invoked in a dispatch * thread which is separate from the {@link Connection}'s thread. This * allows {@link Consumer}s to call {@link Channel} or {@link * Connection} methods without causing a deadlock. * &lt;p/> * The {@link Consumer}s on a particular {@link Channel} are invoked serially on one or more * dispatch threads. {@link Consumer}s should avoid executing long-running code * because this will delay dispatch of messages to other {@link Consumer}s on the same * {@link Channel}. * * @see Channel#basicConsume(String, boolean, String, boolean, boolean, java.util.Map, Consumer) * @see Channel#basicCancel */ public interface Consumer { /** * Called when the consumer is registered by a call to any of the * {@link Channel#basicConsume} methods. * @param consumerTag the &lt;i>consumer tag&lt;/i> associated with the consumer */ void handleConsumeOk(String consumerTag); /** * Called when the consumer is cancelled by a call to {@link Channel#basicCancel}. * @param consumerTag the &lt;i>consumer tag&lt;/i> associated with the consumer */ void handleCancelOk(String consumerTag); /** * Called when the consumer is cancelled for reasons &lt;i>other than&lt;/i> by a call to * {@link Channel#basicCancel}. For example, the queue has been deleted. * See {@link #handleCancelOk} for notification of consumer * cancellation due to {@link Channel#basicCancel}. * @param consumerTag the &lt;i>consumer tag&lt;/i> associated with the consumer * @throws IOException */ void handleCancel(String consumerTag) throws IOException; /** * Called when either the channel or the underlying connection has been shut down. * @param consumerTag the &lt;i>consumer tag&lt;/i> associated with the consumer * @param sig a {@link ShutdownSignalException} indicating the reason for the shut down */ void handleShutdownSignal(String consumerTag, ShutdownSignalException sig); /** * Called when a &lt;code>&lt;b>basic.recover-ok&lt;/b>&lt;/code> is received * in reply to a &lt;code>&lt;b>basic.recover&lt;/b>&lt;/code>. All messages * received before this is invoked that haven't been &lt;i>ack&lt;/i>'ed will be * re-delivered. All messages received afterwards won't be. * @param consumerTag the &lt;i>consumer tag&lt;/i> associated with the consumer */ void handleRecoverOk(String consumerTag); /** * Called when a &lt;code>&lt;b>basic.deliver&lt;/b>&lt;/code> is received for this consumer. * @param consumerTag the &lt;i>consumer tag&lt;/i> associated with the consumer * @param envelope packaging data for the message * @param properties content header data for the message * @param body the message body (opaque, client-specific byte array) * @throws IOException if the consumer encounters an I/O error while processing the message * @see Envelope */ void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException; } 继承com.rabbitmq.client.DefaultConsumer类 import java.io.IOException; /** * Convenience class providing a default implementation of {@link Consumer}. * We anticipate that most Consumer implementations will subclass this class. */ public class DefaultConsumer implements Consumer { /** Channel that this consumer is associated with. */ private final Channel _channel; /** Consumer tag for this consumer. */ private volatile String _consumerTag; /** * Constructs a new instance and records its association to the passed-in channel. * @param channel the channel to which this consumer is attached */ public DefaultConsumer(Channel channel) { _channel = channel; } /** * Stores the most recently passed-in consumerTag - semantically, there should be only one. * @see Consumer#handleConsumeOk */ public void handleConsumeOk(String consumerTag) { this._consumerTag = consumerTag; } /** * No-op implementation of {@link Consumer#handleCancelOk}. * @param consumerTag the defined consumer tag (client- or server-generated) */ public void handleCancelOk(String consumerTag) { // no work to do } /** * No-op implementation of {@link Consumer#handleCancel(String)} * @param consumerTag the defined consumer tag (client- or server-generated) */ public void handleCancel(String consumerTag) throws IOException { // no work to do } /** * No-op implementation of {@link Consumer#handleShutdownSignal}. */ public void handleShutdownSignal(String consumerTag, ShutdownSignalException sig) { // no work to do } /** * No-op implementation of {@link Consumer#handleRecoverOk}. */ public void handleRecoverOk(String consumerTag) { // no work to do } /** * No-op implementation of {@link Consumer#handleDelivery}. */ public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { // no work to do } /** * Retrieve the channel. * @return the channel this consumer is attached to. */ public Channel getChannel() { return _channel; } /** * Retrieve the consumer tag. * @return the most recently notified consumer tag. */ public String getConsumerTag() { return _consumerTag; } } 消费端自定义监听代码演示消费端自定义监听 import com.rabbitmq.client.*; import java.io.IOException; /** * @Classname MyConsumer * @Description 自定义消费端 * @Date 2019/9/9 16:03 * @Created by Jiavg */ public class MyConsumer extends DefaultConsumer { private Channel channel; /** * Constructs a new instance and records its association to the passed-in channel. * @param channel the channel to which this consumer is attached */ public MyConsumer(Channel channel) { super(channel); this.channel = channel; } @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { // super.handleDelivery(consumerTag, envelope, properties, body); System.out.println(\"consumerTag:\" + consumerTag + \"--此条消息已处理\"); System.out.println(\"\\t消息内容:\" + new String(body)); channel.basicAck(envelope.getDeliveryTag(), false); } } 消费端代码实现 import com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; /** * @Classname Consumer * @Description 消费端自定义监听：消费端 * @Date 2019/9/8 10:48 * @Created by Jiavg */ public class Consumer { public static void main(String[] args) throws Exception{ ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(\"127.0.0.1\"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost(\"/\"); connectionFactory.setUsername(\"guest\"); connectionFactory.setPassword(\"guest\"); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); String exchangeName = \"test_Custom_Exchange\"; String routingKey = \"Custom.#\"; String queueName = \"test_Custom_Queue\"; // 声明交换机、声明队列、绑定队列到交换机 channel.exchangeDeclare(exchangeName, \"topic\", true, false, null); channel.queueDeclare(queueName, true, false, false, null); channel.queueBind(queueName, exchangeName, routingKey); MyConsumer myConsumer = new MyConsumer(channel); channel.basicConsume(queueName, false, myConsumer); } } 生产端代码实现 import com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; /** * @Classname Producer * @Description 消费端自定义监听：生产端 * @Date 2019/9/8 10:50 * @Created by Jiavg */ public class Producer { public static void main(String[] args) throws Exception{ ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(\"127.0.0.1\"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost(\"/\"); connectionFactory.setUsername(\"guest\"); connectionFactory.setPassword(\"guest\"); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); String exchangeName = \"test_Custom_Exchange\"; String routingKey = \"Custom.save\"; String msg = \"Hello World send Custom Message\"; for (int i = 0; i &lt; 4; i++){ channel.basicPublish(exchangeName, routingKey, null, msg.getBytes()); } } } 消费端控制台变换","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/categories/消息中间件/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://lylgjiavg.github.io/tags/RabbitMQ/"},{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/tags/消息中间件/"}]},{"title":"RabbitMQ-消费端限流","slug":"RabbitMQ09","date":"2019-09-08T04:33:11.000Z","updated":"2019-09-08T06:22:57.820Z","comments":true,"path":"2019/09/08/RabbitMQ09/","link":"","permalink":"https://lylgjiavg.github.io/2019/09/08/RabbitMQ09/","excerpt":"","text":"消费端限流什么是消费端限流 假设一个场景,首先,我们RabbitMQ服务器上有上万条为处理的消息,我们随便打开一个消费者客户端,会出现下面的情况: 巨量的消息瞬间全部推送过来,但是我们的单个客户端无法同时处理这么多数据,造成客户端宕机! RabbitMQ消费端限流机制 RabbitMQ提供了一种QoS(服务质量保证)功能,即在非自动确认消息的前提下,如果一定数目的消息(通过基于Consume或者Channel设置的QoS值)未被确认前,不进行消费新的消息. void basicQos(int prefetchSize, int prefetchCount, boolean global) throws IOException; prefetchSize: 服务器将交付的最大内容量(以八进制为单位度量)，如果没有限制，则设置为0 prefetchCount: 会告诉RabbitMQ不要同时给一个消费者推送多于prefetchCount个消息,即一旦有prefetchCount个消息未ack,则该Consumer将block掉,直到有消息ack. global:true/false是否将上面的设置应用于Channel;简单点说,就是上面限制是Channel级别还是Consumer级别. prefetchSize和global这两项,RabbitMQ没有实现,暂且不研究prefetchCount在no_ack=false的情况下生效,即在自动应答的情况下这两个值是不生效的. RabbitMQ消费端限流代码演示自定义消费者 import com.rabbitmq.client.AMQP; import com.rabbitmq.client.Channel; import com.rabbitmq.client.DefaultConsumer; import com.rabbitmq.client.Envelope; import java.io.IOException; /** * @Classname MyConsumer * @Description TODO * @Date 2019/9/8 13:15 * @Created by Jiavg */ public class MyConsumer extends DefaultConsumer { /** * Constructs a new instance and records its association to the passed-in channel. * @param channel the channel to which this consumer is attached */ public MyConsumer(Channel channel) { super(channel); this.channel = channel; } private Channel channel; @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { System.out.println(\"---------consume message---------\"); System.out.println(\"consumerTag:\" + consumerTag); System.out.println(\"envelope:\" + envelope); System.out.println(\"properties:\" + properties); System.out.println(\"body:\" + new String(body)); // 延时ack,便于演示消费端限流 try { Thread.sleep(10000); } catch (InterruptedException e) { e.printStackTrace(); } channel.basicAck(envelope.getDeliveryTag(), false); } } 消费端代码 import com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; /** * @Classname Consumer * @Description 消费端限流：消费者 * @Date 2019/9/8 10:48 * @Created by Jiavg */ public class Consumer { public static void main(String[] args) throws Exception{ ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(\"127.0.0.1\"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost(\"/\"); connectionFactory.setUsername(\"guest\"); connectionFactory.setPassword(\"guest\"); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); String exchangeName = \"test_QoS_Exchange\"; String routingKey = \"QoS.#\"; String queueName = \"test_QoS_Queue\"; // 声明交换机、声明队列、绑定队列到交换机 channel.exchangeDeclare(exchangeName, \"topic\", true, false, null); channel.queueDeclare(queueName, true, false, false, null); channel.queueBind(queueName, exchangeName, routingKey); // 限流方式 channel.basicQos(0, 1, false); // autoAck设置为false channel.basicConsume(queueName, false, new MyConsumer(channel)); } } 启动消费端-管控台变化 交换机变化 交换机绑定情况 生产端代码 import com.rabbitmq.client.*; /** * @Classname Producer * @Description 消费端限流：生产者 * @Date 2019/9/8 10:50 * @Created by Jiavg */ public class Producer { public static void main(String[] args) throws Exception{ ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(\"127.0.0.1\"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost(\"/\"); connectionFactory.setUsername(\"guest\"); connectionFactory.setPassword(\"guest\"); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); String exchangeName = \"test_QoS_Exchange\"; String routingKey = \"QoS.save\"; String msg = \"Hello World send QoS Message\"; for (int i = 0; i &lt; 4; i++){ channel.basicPublish(exchangeName, routingKey, null, msg.getBytes()); } } } 生产端运行-管控台变化 Overview页面变化 由图可知,由于我们自定义消费者在处理消息的方法中(handleDelivery),使用Thread.sleep(10000);延迟10秒ack,故在Overview页面显示每10秒才会消费一条消息(只有在Broker接收到消费端的ack才会发送下一条消息).","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/categories/消息中间件/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://lylgjiavg.github.io/tags/RabbitMQ/"},{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/tags/消息中间件/"}]},{"title":"RabbitMQ-Return消息机制","slug":"RabbitMQ08","date":"2019-09-08T02:28:53.000Z","updated":"2019-09-08T03:42:59.636Z","comments":true,"path":"2019/09/08/RabbitMQ08/","link":"","permalink":"https://lylgjiavg.github.io/2019/09/08/RabbitMQ08/","excerpt":"","text":"RabbitMQ的Return消息机制Return消息机制的作用 Return Listener用于处理一些不可路由的消息. 我们的消息生产者,通过指定一个Exchange和RoutingKey,把消息送达到某一个队列中去,然后我们的消费者监听队列,进行消费处理操作. 但是在某些情况下,如果我们发送息的时消候,当前的Exchange不存在或者指定的路由Key路由不到,这个时候如果我们需要监听这种不可到达的消息,就需要使用Return Listener. 启用Return消息机制的关键配置 在基础API中有一个关键的配置项:Mandatory:如果为true,则监听器会接收到路由不可到达的消息,然后进行后续处理;如果为false,那么broker端会自动删除该消息. Return消息机制流程 Return消息机制流程图如下所示: Return消息机制代码演示消费端实现消费者代码 import com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; import com.rabbitmq.client.QueueingConsumer; /** * @Classname Consumer * @Description Return消息机制：消费者 * @Date 2019/9/8 10:48 * @Created by Jiavg */ public class Consumer { public static void main(String[] args) throws Exception{ ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(\"127.0.0.1\"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost(\"/\"); connectionFactory.setUsername(\"guest\"); connectionFactory.setPassword(\"guest\"); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); String exchangeName = \"test_Return_Exchange\"; String routingKey = \"return.#\"; String queueName = \"test_Return_Queue\"; // 声明交换机、声明队列、绑定队列到交换机 channel.exchangeDeclare(exchangeName, \"topic\", true, false, null); channel.queueDeclare(queueName, true, false, false, null); channel.queueBind(queueName, exchangeName, routingKey); // 创建消费者、消费队列 QueueingConsumer queueingConsumer = new QueueingConsumer(channel); channel.basicConsume(queueName, true, queueingConsumer); while (true) { QueueingConsumer.Delivery delivery = queueingConsumer.nextDelivery(); byte[] body = delivery.getBody(); System.out.printf(\"消费端:\" + new String(body)); } } } 启动消费端-管控台变化 交换机变化 交换机绑定情况 注意:此交换机与队列绑定的路由Key我们在生产者端时并不遵守此路由key,即让消息无法正确路由,以触发ReturnListener. 生产端实现生产者代码 import com.rabbitmq.client.*; import java.io.IOException; /** * @Classname Producer * @Description Return消息机制：生产者 * @Date 2019/9/8 10:50 * @Created by Jiavg */ public class Producer { public static void main(String[] args) throws Exception{ ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(\"127.0.0.1\"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost(\"/\"); connectionFactory.setUsername(\"guest\"); connectionFactory.setPassword(\"guest\"); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); String exchangeName = \"test_Return_Exchange\"; String routingKey = \"unReturn.save\"; String msg = \"Hello World send Return Message\"; // 添加一个Return监听 channel.addReturnListener(new ReturnListener() { @Override public void handleReturn(int replyCode, String replyText, String exchange, String routingKey, AMQP.BasicProperties properties, byte[] body) throws IOException { System.out.println(\"----------handle return-----------\"); System.out.println(\"replyCode:\" + replyCode); System.out.println(\"replyText:\" + replyText); System.out.println(\"exchange:\" + exchange); System.out.println(\"routingKey:\" + routingKey); System.out.println(\"properties:\" + properties); System.out.println(\"body:\" + new String(body)); } }); // 发送消息:第三个参数mandatory:true,默认为false channel.basicPublish(exchangeName, routingKey, true, null, msg.getBytes()); } } 注意:消息的路由key我们设置为unReturn.save,在相应交换机上并没有符合此路由key的队列,此条消息将无法正确路由到队列上. 启动生产端-管控台变化 Overview页面变化 由上图可知,虽然RabbitMQ接收到了一条消息(Message rates发生变化),但是并没有被路由到正确队列中(Queued messages的Total为0). 生产者控制台变化 可见,由于消息无法正确路由到相应的队列中,触发了Return Listener监听器(打印了相应的信息). 注意:要使RabbitMQ回送给生产者无法路由的消息,在生产者端basicPublish(String exchange, StringroutingKey, boolean mandatory, BasicProperties props, byte[] body) throws IOException;的mandatory要设置为true,否则Broker端默认会自动删除该消息.","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/categories/消息中间件/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://lylgjiavg.github.io/tags/RabbitMQ/"},{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/tags/消息中间件/"}]},{"title":"RabbitMQ-Confirm确认消息","slug":"RabbitMQ07","date":"2019-09-07T02:10:12.000Z","updated":"2019-09-07T03:57:31.620Z","comments":true,"path":"2019/09/07/RabbitMQ07/","link":"","permalink":"https://lylgjiavg.github.io/2019/09/07/RabbitMQ07/","excerpt":"","text":"Confirm消息确认机制 消息的确认,是指生产者投递消息后,如果Broker接收到消息,则会给我们生产者一个应答. 生产者进行接收应答,用来确认这条消息是否正常的发送到Broker,这种方式也是消息的可靠性投递的核心保障. Confirm确认消息流程解析 确认机制流程图 如何实现Confirm确认消息? Step1:在channel上开启确认模式:channel.confirmSelect(); Step2:在Channel上添加监听:addConfirmListener,监听成功和失败的返回结果,根据具体的结果对消息进行重新发送、或记录日志等后续处理。 Confirm确认消息代码演示消费者实现消费者代码 import com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; import com.rabbitmq.client.QueueingConsumer; /** * @Classname Consumer * @Description Confirm确认机制：消费者 * @Date 2019/9/7 10:48 * @Created by Jiavg */ public class Consumer { public static void main(String[] args) throws Exception{ ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(\"127.0.0.1\"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost(\"/\"); connectionFactory.setUsername(\"guest\"); connectionFactory.setPassword(\"guest\"); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); String exchangeName = \"test_Confirm_Exchange\"; String routingKey = \"confirm.#\"; String queueName = \"test_Confirm_Queue\"; // 声明交换机、声明队列、绑定队列到交换机 channel.exchangeDeclare(exchangeName, \"topic\", true, false, null); channel.queueDeclare(queueName, true, false, false, null); channel.queueBind(queueName, exchangeName, routingKey); // 创建消费者、消费队列 QueueingConsumer queueingConsumer = new QueueingConsumer(channel); channel.basicConsume(queueName, true, queueingConsumer); while (true) { QueueingConsumer.Delivery delivery = queueingConsumer.nextDelivery(); byte[] body = delivery.getBody(); System.out.printf(\"消费端:\" + new String(body)); } } } 消费者运行-管控台变化 新增对应交换机 新增对应队列 队列绑定关系 生产者实现生产者代码 import com.rabbitmq.client.Channel; import com.rabbitmq.client.ConfirmListener; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; import java.io.IOException; /** * @Classname Producer * @Description Confirm确认机制：生产者 * @Date 2019/9/7 10:42 * @Created by Jiavg */ public class Producer { public static void main(String[] args) throws Exception{ ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(\"127.0.0.1\"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost(\"/\"); connectionFactory.setUsername(\"guest\"); connectionFactory.setPassword(\"guest\"); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); // 指定消息投递模式:消息的确认模式 channel.confirmSelect(); String exchangeName = \"test_Confirm_Exchange\"; String routingKey = \"confirm.save\"; String msg = \"Hello World send Confirm Message\"; // 发送消息 channel.basicPublish(exchangeName, routingKey, null, msg.getBytes()); // 添加一个确认监听 channel.addConfirmListener(new ConfirmListener() { @Override public void handleAck(long deliveryTag, boolean multiple) throws IOException { System.out.printf(\"-------Ack-------\"); } @Override public void handleNack(long deliveryTag, boolean multiple) throws IOException { System.out.printf(\"------No Ack-----\"); } }); } } 生产者运行-控制台变化 消费端接收到监听队列的消息. 生产端的确认监听方法打印出相应信息. 注意事项Point1 Confirm确认机制是生产者和Broker(RabbitMQ)之间的确认机制,不涉及到消费者是否消费了此条消息.为了更好的理解,我们进行了如下实验: 紧接着上面的实验:我们关闭消费者端,并重新运行生产者端: 管控台变化 通过上图我们可以看出由于关闭了消费者端,该条信息并未被消费,故存留在RabbitMQ的队列中. 生产者控制台变化 通过上图可以看出,Broker给生产者发送了Ack消息确认.由此可见,只要是Broker正确的接收到消息就会给生产端发送Ack确认,而不管消费端是否正常消费了此条消息. Point2 Confirm确认机制是生产者和Broker(RabbitMQ)之间的确认机制,甚至Broker没有将此消息正确路由到对应的队列时,还是会给生产者端发送Ack确认.为了更好的理解,我们进行了如下实验: 紧接着上面的实验,我们修改生产者的发送消息的路由键为”unConfirm.save”(此时RabbitMQ中并没有对应的队列),此时我们运行生产端. 管控台变化 通过上图我们可以看出由于修改了生产者的消息的路由键,此时虽然Message Rates虽然发生改变(Broker接收到消息),但是并没有将此条消息正确的路由到对应的队列中(Queued Messages的总消息数量没有发生改变). 生产者控制台变化 通过上图可以看出,Broker给生产者发送了Ack消息确认.由此可见,虽然消息并未正确的路由到队列中,但是Broker接收到了消息就会给生产者端发送Ack确认. Confirm确认监听情况说明 什么时候会走 handleNack 方法呢，比如磁盘写满了，MQ出现了一些异常，或者Queue容量到达上限了等等 也有可能两个方法都不走，比如生产端发送消息就失败了，或者Broker端收到消息在返回ack时中途出现了网络闪断。 这种情况就需要定时任务去抓取中间状态的消息进行最大努力尝试次数的补偿重发，从而保障消息投递的可靠性。","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/categories/消息中间件/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://lylgjiavg.github.io/tags/RabbitMQ/"},{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/tags/消息中间件/"}]},{"title":"生产端的可靠性投递","slug":"RabbitMQ06","date":"2019-09-04T12:59:07.000Z","updated":"2019-09-04T14:17:40.851Z","comments":true,"path":"2019/09/04/RabbitMQ06/","link":"","permalink":"https://lylgjiavg.github.io/2019/09/04/RabbitMQ06/","excerpt":"","text":"消息如何保证100%的投递成功?什么是生产端的可靠性投递? 保障消息的成功发出 保障MQ节点的成功接收 发送端收到MQ节点(Broker)确认应答 完善的消息进行补偿机制 生产端可靠性投递——之BAT/TMD互联网大厂的解决方案 消息落库,对消息状态进行打标 消息的延迟投递,做二次确认,回调检查 具体使用哪种要根据业务场景和并发量、数据量大小来决定 方案一: 消息信息落库,对消息状态进行打标 消息信息落库，对消息状态进行打标的方案如下图所示： 具体步骤如下: Step 1：进行业务数据入库：比如发送一条订单消息，首先把业务数据也就是订单信息进行入库，然后生成一条消息，把消息也进行入库，这条消息应该包含消息状态属性，并设置初始值比如为0，表示消息创建成功正在发送中，这种方式缺陷在于我们要对数据库进行持久化两次。 Step 2:首先要保证第一步消息都存储成功了，没有出现任何异常情况，然后生产端再进行消息发送。如果失败了就进行快速失败机制。 Step 3:MQ把消息收到的结果应答(confirm)给生产端。 Step 4:生产端有一个Confirm Listener，去异步的监听Broker回送的响应，从而判断消息是否投递成功，如果成功，去数据库查询该消息，并将消息状态更新为1，表示消息投递成功。 假设step 2 已经OK了，在第三步回送响应时，网络突然出现了闪断，导致生产端的Listener收不到这条消息的confirm应答，也就是说这条消息的状态一直为0了。 Step 5:此时我们需要设置一个规则，比如说消息在入库时候设置一个临界值timeout，5分钟之后如果状态还是0，那就需要把消息抽取出来。这里,使用分布式定时任务，去定时抓取DB中距离消息创建时间超过5分钟的且状态为0的消息。 Step 6:把抓取出来的消息进行重新投递(Retry Send)，也就是从Step 2开始继续往下走。 Step 7:当然有些消息可能由于一些实际的问题无法路由到Broker，比如routingKey设置不对，对应的队列被误删除了，这种消息即使重试多次也仍然无法投递成功，所以需要对重试次数做限制，比如限制3次，如果投递次数大于3次，那么就将消息状态更新为2，表示这个消息最终投递失败。 本方案的局限性:对于本方案，需要做两次数据库的持久化操作，在高并发场景下数据库将存在性能瓶颈。其实在核心链路中只需要对业务数据进行入库，消息没必要先入库，可以做一个消息的延迟投递，做二次确认，回调检查。 方案二:消息的延迟投递，做二次确认，回调检查 消息的延迟投递，做二次确认，回调检查的方案如下图所示: 具体步骤如下: Upstream Service(上游服务,即:生产端)，Downstream service(下游服务即:消费端)，Callback service(回调服务)。 Step1：先将业务消息进行入库，然后生产端将消息发送出去，注意一定是等数据库操作完成:之后再去发送消息。 Step 2：在发送消息之后，紧接着生产端再次发送一条消息(Second Send Delay Check)，即延迟消息投递检查，这里需要设置一个延迟时间，比如5分钟之后进行投递。 Step 3：消费端去监听指定队列，将收到的消息进行处理。 Step 4：处理完成之后，发送一个confirm消息，也就是回送响应，但是这里响应不是正常的ACK，而是重新生成一条消息，投递到MQ中。 Step 5:上面的Callback service是一个单独的服务，其实它扮演了方案一的存储消息的DB角色，它通过MQ去监听下游服务发送的confirm消息，如果Callback service收到confirm消息，那么就对消息做持久化存储，即将消息持久化到DB中。 Step6：5分钟之后延迟消息发送到MQ了，然后Callback service还是去监听延迟消息所对应的队列，收到Check消息后去检查DB中是否存在消息，如果存在，则不需要做任何处理，如果不存在或者消费失败了，那么Callback service就需要主动发起RPC通信给上游服务，告诉它延迟投递的这条消息没有找到，需要重新发送，生产端收到信息后就会重新查询业务消息然后将消息发送出去。 本方案的优势与劣势: 方案二也是互联网大厂更为经典和主流的解决方案 方案二不一定能保障百分百投递成功，但是基本上可以保障大概99.9%的消息是OK的，有些特别极端的情况只能是人工去做补偿了，或者使用定时任务去做。 方案二主要目的是为了减少数据库操作，提高并发量。 在高并发场景下，最关心的不是消息100%投递成功，而是一定要保证性能，保证能抗得住这么大的并发量。所以能减少数据库的操作就尽量减少，可以异步的进行补偿。 其实在主流程里面是没有这个Callback service的，它属于一个补偿的服务，整个核心链路就是生产端入库业务消息，发送消息到MQ，消费端监听队列，消费消息。其他的步骤都是一个补偿机制。 消费端-幂等性保障 在海量订单产出的业务高峰期，如何避免消息的重复消费问题？ 消费端实现幂等性，就意味着，我们的消息永远不会消费多次，即使我们收到多条一样的消息 主流的幂等性操作 唯一ID+指纹码 ，利用数据库主键去重好处：实现简单坏处：高并发下有DB写入的性能瓶颈解决方案：跟进ID进行分库分表进行算法路由 利用Redis的原子性去实现幂等,需要考虑的问题?1、我们是否进行数据落库，如果落库的话，关键解决的问题是数据库和缓存如何做到原子性?2、如果不落库，存储到缓存中，如何设置定时同步的策略? 深入理解幂等性###什么是幂等性 HTTP/1.1中对幂等性的定义是：一次和多次请求某一个资源对于资源本身应该具有同样的结果（网络超时等问题除外）。也就是说，其任意多次执行对资源本身所产生的影响均与一次执行的影响相同。 Methods can also have the property of “idempotence” in that (aside from error orexpiration issues) the side-effects of N &gt; 0 identical requests is the same as for asingle request. 这里需要关注几个重点： 幂等不仅仅只是一次（或多次）请求对资源没有副作用（比如查询数据库操作，没有增删改，因此没有对数据库有任何影响）。 幂等还包括第一次请求的时候对资源产生了副作用，但是以后的多次请求都不会再对资源产生副作用。 幂等关注的是以后的多次请求是否对资源产生的副作用，而不关注结果。 网络超时等问题，不是幂等的讨论范围。 幂等性是系统服务对外一种承诺（而不是实现），承诺只要调用接口成功，外部多次调用对系统的影响是一致的。声明为幂等的服务会认为外部调用失败是常态，并且失败之后必然会有重试。 什么情况下需要幂等 业务开发中，经常会遇到重复提交的情况，无论是由于网络问题无法收到请求结果而重新发起请求，或是前端的操作抖动而造成重复提交情况。 在交易系统，支付系统这种重复提交造成的问题有尤其明显，比如： 用户在APP上连续点击了多次提交订单，后台应该只产生一个订单； 向支付宝发起支付请求，由于网络问题或系统BUG重发，支付宝应该只扣一次钱。 很显然，声明幂等的服务认为，外部调用者会存在多次调用的情况，为了防止外部多次调用对系统数据状态的发生多次改变，将服务设计成幂等。 幂等VS防重上面例子中遇到的问题，只是重复提交的情况，和服务幂等的初衷是不同的。重复提交是在第一次请求已经成功的情况下，人为的进行多次操作，导致不满足幂等要求的服务多次改变状态。而幂等更多使用的情况是第一次请求不知道结果（比如超时）或者失败的异常情况下，发起多次请求，目的是多次确认第一次请求成功，却不会因多次请求而出现多次的状态变化。 什么情况下需要保证幂等性 以SQL为例，有下面三种场景，只有第三种场景需要开发人员使用其他策略保证幂等性： SELECT col1 FROM tab1 WHER col2=2，无论执行多少次都不会改变状态，是天然的幂等。 UPDATE tab1 SET col1=1 WHERE col2=2，无论执行成功多少次状态都是一致的，因此也是幂等操作。 UPDATE tab1 SET col1=col1+1 WHERE col2=2，每次执行的结果都会发生变化，这种不是幂等的。 为什么要设计幂等性的服务幂等可以使得客户端逻辑处理变得简单，但是却以服务逻辑变得复杂为代价。满足幂等服务的需要在逻辑中至少包含两点： 首先去查询上一次的执行状态，如果没有则认为是第一次请求 在服务改变状态的业务逻辑前，保证防重复提交的逻辑 幂等的不足 幂等是为了简化客户端逻辑处理，却增加了服务提供者的逻辑和成本，是否有必要，需要根据具体场景具体分析，因此除了业务上的特殊要求外，尽量不提供幂等的接口。 增加了额外控制幂等的业务逻辑，复杂化了业务功能； 把并行执行的功能改为串行执行，降低了执行效率。 保证幂等策略 幂等需要通过唯一的业务单号来保证。也就是说相同的业务单号，认为是同一笔业务。使用这个唯一的业务单号来确保，后面多次的相同的业务单号的处理逻辑和执行效果是一致的。 下面以支付为例，在不考虑并发的情况下，实现幂等很简单：①先查询一下订单是否已经支付过，②如果已经支付过，则返回支付成功；如果没有支付，进行支付流程，修改订单状态为‘已支付’。 防重复提交策略 上述的保证幂等方案是分成两步的，第②步依赖第①步的查询结果，无法保证原子性的。在高并发下就会出现下面的情况：第二次请求在第一次请求第②步订单状态还没有修改为‘已支付状态’的情况下到来。既然得出了这个结论，余下的问题也就变得简单：把查询和变更状态操作加锁，将并行操作改为串行操作。 乐观锁如果只是更新已有的数据，没有必要对业务进行加锁，设计表结构时使用乐观锁，一般通过version来做乐观锁，这样既能保证执行效率，又能保证幂等。例如： UPDATE tab1 SET col1=1,version=version+1WHERE version=#version# 不过，乐观锁存在失效的情况，就是常说的ABA问题，不过如果version版本一直是自增的就不会出现ABA的情况。（从网上找了一张图片很能说明乐观锁，引用过来，出自Mybatis对乐观锁的支持） 防重表使用订单号orderNo做为去重表的唯一索引，每次请求都根据订单号向去重表中插入一条数据。第一次请求查询订单支付状态，当然订单没有支付，进行支付操作，无论成功与否，执行完后更新订单状态为成功或失败，删除去重表中的数据。后续的订单因为表中唯一索引而插入失败，则返回操作失败，直到第一次的请求完成（成功或失败）。可以看出防重表作用是加锁的功能。 分布式锁这里使用的防重表可以使用分布式锁代替，比如Redis。订单发起支付请求，支付系统会去Redis缓存中查询是否存在该订单号的Key，如果不存在，则向Redis增加Key为订单号。查询订单支付已经支付，如果没有则进行支付，支付完成后删除该订单号的Key。通过Redis做到了分布式锁，只有这次订单订单支付请求完成，下次请求才能进来。相比去重表，将放并发做到了缓存中，较为高效。思路相同，同一时间只能完成一次支付请求。 token令牌这种方式分成两个阶段：申请token阶段和支付阶段。 第一阶段，在进入到提交订单页面之前，需要订单系统根据用户信息向支付系统发起一次申请token的请求，支付系统将token保存到Redis缓存中，为第二阶段支付使用。 第二阶段，订单系统拿着申请到的token发起支付请求，支付系统会检查Redis中是否存在该token，如果存在，表示第一次发起支付请求，删除缓存中token后开始支付逻辑处理；如果缓存中不存在，表示非法请求。 实际上这里的token是一个信物，支付系统根据token确认，你是你妈的孩子。不足是需要系统间交互两次，流程较上述方法复杂。 支付缓冲区把订单的支付请求都快速地接下来，一个快速接单的缓冲管道。后续使用异步任务处理管道中的数据，过滤掉重复的待支付订单。优点是同步转异步，高吞吐。不足是不能及时地返回支付结果，需要后续监听支付结果的异步返回。","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/categories/消息中间件/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://lylgjiavg.github.io/tags/RabbitMQ/"},{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/tags/消息中间件/"}]},{"title":"RabbitMQ-基本概念","slug":"RabbitMQ05","date":"2019-09-04T07:28:00.000Z","updated":"2019-09-04T08:52:33.040Z","comments":true,"path":"2019/09/04/RabbitMQ05/","link":"","permalink":"https://lylgjiavg.github.io/2019/09/04/RabbitMQ05/","excerpt":"","text":"RabbitMQ的基本概念Binding-绑定 Exchange和Exchange、Queue之间的连接关系 Binding中可以包含Routing Key或则参数 注意:default Exchange 不能进行Binding，也不需要进行绑定除default Exchange之外，其他任何Exchange都需要和Queue进行Binding，否则无法进行消息路由（转发）Binding的时候，可以设置一个或多个参数，其中参数要特别注意参数类型Direct Exchange、Topic Exchange进行Binding的时候，需要指定Routing keyFanout Exchange、Headers Exchange进行Binding的时候，不需要指定Routing key Queue-消息队列 消息队列，实际存储消息数据 Durability：是否持久化，Durable：是，Transient：否 Auto delete：如选yes，代表当最后一个监听被移走之后，该Queue会自动删除 注意: 关于Queue名称最长？RabbitMQ规定，队列的名字最长不超过UTF-8编码的255字节。不过一般没人无聊到声明名字那么长的队列吧。。。 关于Queue名称雷区？由于RabbitMQ内部的Queue命名规则采用 “amq.”形式，所以当我们声明自己的Queue时，注意不要与此规则冲突，否则会报异常。 关于队列属性？Durable ：代表该队列是否持久化至硬盘（若要使队列中消息不丢失，同时也需要将消息声明为持久化）；Exclusive :是否声明该队列是否为连接独占，若为独占，连接关闭后队列即被删除；Auto-delete：若没有消费者订阅该队列，队列将被删除；Arguments：可选map类型参数，可以指定队列长度，消息生存时间，镜相设置等 声明了一个已经存在的队列？如果队列已经存在，再次声明将不会起作用。若原始队列参数和该次声明时不同则会报异常。 队列中消息顺序？默认情况下是FIFO，即先进先出，同时也支持发送消息时指定消息的优先级。 队列消息存放位置？对于临时消息，RabbitMQ尽量将其存放在内存，当出现内存告警时，MQ会将消息持久化至硬盘。对于持久化消息与Lazy-queues，MQ会先将消息存入硬盘，消费时再取出。 队列中消息的消费？默认情况下，MQ会设置消费者的消费确认模式为自动。对于一些重要消息的处理，推荐确认模式改为手动。（nack和reject区别？nack可以一次拒绝多条消息） 队列中消息的消费速度？通过Prefetch（通道上最大未确认投递数量）设置消费者每次消费的条数，一般将该值设为1，但他会降低吞吐量。RabbitMQ官网建议的是100-300.（更建议反复试验得到一个表现符合期望的值） 队列中消息状态？队列中的消息共有俩种状态，一是准备投递，二是已投递但未确认。 队列最大长度？声明队列时可以指定最大长度，需要注意的是只限制状态为准备投递的数量，未确认的消息不计算在内。当队列长度超过限制，MQ会根据策略选择丢弃（默认）或者将消息投递进死信队列。 关于死信队列？其实更准确的说法是死信交换机，提前声明一个交换机，在声明队列时使用“x-dead-letter-exchange”参数（可指定routKey）将队列绑定到该死信交换机。消息有以下情况之一会成为死信：被reject或者nack，消息超过生存时间，队列长度超过限制。 关于不能路由到队列的消息？这个和上面一样，其实不算Queue系列而是Exchange。针对消息无法路由到队列的情况MQ提供了AlternateExchange处理。声明Exchange时添加args.put(“alternate-exchange”,”my-ae”)参数。即当该交换机存在无法路由的消息时，它将消息发布到AE上，AE把消息路由到绑定在他上面的消息。 Message-消息 服务器和应用程序之间传送的数据 本质上就是一段数据，由Properties和Payload（Body）组成 发送消息可以为消息指定一些参数Delivery mode: 是否持久化，1 - Non-persistent，2 - PersistentHeaders：Headers can have any name. Only long string headers can be set here.Properties: You can set other message properties here (delivery mode and headers arepulled out as the most common cases). Invalid properties will be ignored.Valid properties are: content_type ： 消息内容的类型 content_encoding： 消息内容的编码格式 priority： 消息的优先级 correlation_id：关联id reply_to: 用于指定回复的队列的名称 expiration： 消息的失效时间 message_id： 消息id timestamp：消息的时间戳 type： 类型 user_id: 用户id app_id： 应用程序id cluster_id: 集群id Payload: 消息内容————————————————版权声明：本文为CSDN博主「gmHappy」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/ctwy291314/article/details/83183633 注意: 如何保证消息不丢失 Exchange需要持久化 Queue需要持久化 Message需要持久化 Virtual host-虚拟主机 虚拟地址，用于进行逻辑隔离，最上层的消息路由 一个Virtual Host里面可以有若干个Exchange和Queue 同一个Virtual Host里面不能有相同名称的Exchange和Queue","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/categories/消息中间件/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://lylgjiavg.github.io/tags/RabbitMQ/"},{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/tags/消息中间件/"}]},{"title":"RabbitMQ-Exchange","slug":"RabbitMQ04","date":"2019-09-03T04:03:55.000Z","updated":"2019-09-07T02:45:43.196Z","comments":true,"path":"2019/09/03/RabbitMQ04/","link":"","permalink":"https://lylgjiavg.github.io/2019/09/03/RabbitMQ04/","excerpt":"","text":"Exchanges(交换机) Exchange:接收消息,并根据路由键转发消息所绑定的队列. 下图为RabbitMQ的整体工作示意图: 交换机属性: Name: 交换机名称 Type: 交换机类型:direct,topic,fanout,headers Durability: 是否需要持久化,true为持久化 Auto Delete:当最后一个绑定到ExChange的队列删除后,自动删除该Exchange Internal:当前Exchange是否用于RabbitMQ内部使用,默认为false Arguments:扩展参数,用于扩展AMQP协议自制定化使用 交换机类型 direct处理路由键。需要将一个队列绑定到交换机上，要求该消息与一个特定的路由键完全匹配。这是一个完整的匹配。如果一个队列绑定到该交换机上要求路由键 “abc”，则只有被标记为“abc”的消息才被转发，不会转发abc.def，也不会转发dog.ghi，只会转发abc。 topic将路由键和某模式进行匹配。此时队列需要绑定要一个模式上。符号“#”匹配一个或多个词，符号“”匹配不多不少一个词。因此“abc.#”能够匹配到“abc.def.ghi”，但是“abc.” 只会匹配到“abc.def”。 fanout不处理路由键。你只需要简单的将队列绑定到交换机上。一个发送到交换机的消息都会被转发到与该交换机绑定的所有队列上。很像子网广播，每台子网内的主机都获得了一份复制的消息。Fanout交换机转发消息是最快的。 headers不处理路由键。而是根据发送的消息内容中的headers属性进行匹配。在绑定Queue与Exchange时指定一组键值对；当消息发送到RabbitMQ时会取到该消息的headers与Exchange绑定时指定的键值对进行匹配；如果完全匹配则消息会路由到该队列，否则不会路由到该队列。headers属性是一个键值对，可以是Hashtable，键值对的值可以是任何类型。而fanout，direct，topic 的路由键都需要要字符串形式的。匹配规则x-match有下列两种类型：x-match = all ：表示所有的键值对都匹配才能接受到消息x-match = any ：表示只要有键值对匹配就能接受到消息 Direct Exchange 所有发送到Direct Exchange的消息被转发到RouteKey中指定的Queue. 以下为Direct模式下Exchange的工作示意图: 注意:Direct模式可以使用RabbitMQ自带的Exchange:default Exchange,所以不需要将Exchange进行任何绑定(binding)操作,消息传递时,RoutKey必须完全匹配才会被队列接收,否则该消息会被抛弃. Direct Exchange演示消费者代码 消费者代码如下 import com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; import com.rabbitmq.client.QueueingConsumer; /** * @Classname Consumer * @Description RabbitMQ的Direct模式:消费者 * @Date 2019/9/3 13:37 * @Created by Jiavg */ public class Consumer { public static void main(String[] args) throws Exception{ // 1.创建ConnectionFactory ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(\"127.0.0.1\"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost(\"/\"); connectionFactory.setUsername(\"guest\"); connectionFactory.setPassword(\"guest\"); // 2.通过连接工厂获得连接 Connection connection = connectionFactory.newConnection(); // 3.通过连接创建信道 Channel channel = connection.createChannel(); String exchangeName = \"test_Direct_Exchange\"; String exchangeType = \"direct\"; String queueName = \"test_Direct_Queue\"; String routeKey = \"test.direct\"; // 4.声明一个交换机 channel.exchangeDeclare(exchangeName,exchangeType,true, false, false, null); // 5.声明一个队列 channel.queueDeclare(queueName, true, false, false, null); // 6. 建立一个绑定关系 channel.queueBind(queueName, exchangeName, routeKey); // 7.创建一个消费者 QueueingConsumer consumer = new QueueingConsumer(channel); // 8.设置信道参数:队列,是否自动ACK,Consumer channel.basicConsume(queueName, true, consumer); // 9.读取消息 while (true){ QueueingConsumer.Delivery delivery = consumer.nextDelivery(); byte[] body = delivery.getBody(); String msg = new String(body); System.out.println(msg); } } } 消费者运行 在运行消费者后,RabbitMQ的管控台变化如下: Connections视口 Channel视口 Exchanges视口 由上图可以观察到,声明的交换机(“test_Direct_Exchange”)为direct模式. Queues视口 生产者代码 生产者代码如下 import com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; /** * @Classname Producer * @Description RabbitMQ的Direct模式:生产者 * @Date 2019/9/3 13:27 * @Created by Jiavg */ public class Producer { public static void main(String[] args) throws Exception{ // 1.创建ConnectionFactory ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(\"127.0.0.1\"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost(\"/\"); connectionFactory.setUsername(\"guest\"); connectionFactory.setPassword(\"guest\"); // 2.通过连接工厂获得连接 Connection connection = connectionFactory.newConnection(); // 3.通过连接创建信道 Channel channel = connection.createChannel(); // 4.声明交换机名称及路由键 String exchangeName = \"test_Direct_Exchange\"; String routeKey = \"test.direct\"; // 5.发送消息 String msg = \"Hello World RabbitMQ 4 Direct Exchange message...\"; channel.basicPublish(exchangeName, routeKey, null, msg.getBytes()); // 6.关闭资源 channel.close(); connection.close(); } } 生产者运行 在运行生产者代码后,RabbitMQ的管控台界面如下图所示: 并且消费者控制台输出如下 可见,在交换机类型为direct模式时,RabbitMQ的交换机根据路由键将接受到的消息路由到相应的队列中,并被相应的监听消费者消费该消息. Topic Exchange 所有发送到Topic Exchange的消息被转发到所有关心RouteKey中指定的Topic Queue上 Exchange将RouteKey和某Topic进行模糊匹配,此时队列需要绑定一个Topic以下为Topic模式下Exchange的工作示意图:注意:可以使用通配符进行模糊匹配符号: “#”匹配一个或多个词符号: “*”匹配不多不少一个词例如: “log.#”能够匹配到”log.info.oa”“log.*”只会匹配到”log.error” Topic Exchange演示消费者代码 消费者代码如下 import com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; import com.rabbitmq.client.QueueingConsumer; /** * @Classname Consumer * @Description RabbitMQ的Topic模式:消费者 * @Date 2019/9/3 16:02 * @Created by Jiavg */ public class Consumer { public static void main(String[] args) throws Exception{ // 1.创建ConnectionFactory ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(\"127.0.0.1\"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost(\"/\"); connectionFactory.setUsername(\"guest\"); connectionFactory.setPassword(\"guest\"); // 2.通过连接工厂获得连接 Connection connection = connectionFactory.newConnection(); // 3.通过连接创建信道 Channel channel = connection.createChannel(); String exchangeName = \"test_Topic_Exchange\"; String exchangeType = \"topic\"; String routeKey = \"test.*\"; String queueName = \"test_Topic_Queue\"; // 4.声明交换机 channel.exchangeDeclare(exchangeName, exchangeType, true, false,false,null); // 5.声明队列 channel.queueDeclare(queueName, true, false, false, null); // 6.创建消费者 QueueingConsumer queueingConsumer = new QueueingConsumer(channel); // 7.绑定队列到交换机上(利用topic模式的routeKey) channel.queueBind(queueName, exchangeName, routeKey); // 8.设置参数 channel.basicConsume(queueName, true, queueingConsumer); // 8.接收消息 while (true) { QueueingConsumer.Delivery delivery = queueingConsumer.nextDelivery(); byte[] body = delivery.getBody(); String msg = new String(body); System.out.printf(msg); } } } 消费者运行 在运行消费者后,RabbitMQ的管控台变化如下: Exchanges视口 由上图可以观察到,声明的交换机(“test_Topic_Exchange”)为topic模式. 生产者代码 生产者代码如下 import com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; /** * @Classname Producer * @Description RabbitMQ的Topic模式:生产者 * @Date 2019/9/3 15:56 * @Created by Jiavg */ public class Producer { public static void main(String[] args) throws Exception{ // 1.创建ConnectionFactory ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(\"127.0.0.1\"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost(\"/\"); connectionFactory.setUsername(\"guest\"); connectionFactory.setPassword(\"guest\"); // 2.通过连接工厂获得连接 Connection connection = connectionFactory.newConnection(); // 3.通过连接创建信道 Channel channel = connection.createChannel(); String msg1 = \"Hello World Topic Exchange with routeKey for test.hello\"; String msg2 = \"Hello World Topic Exchange with routeKey for test.hello.world\"; String msg3 = \"Hello World Topic Exchange with routeKey for hello.world\"; String routeKey1 = \"test.hello\"; String routeKey2 = \"test.hello.world\"; String routeKey3 = \"hello.world\"; String exchangeName = \"test_Topic_Exchange\"; // 4.发送消息 channel.basicPublish(exchangeName, routeKey1, null, msg1.getBytes()); channel.basicPublish(exchangeName, routeKey2, null, msg2.getBytes()); channel.basicPublish(exchangeName, routeKey3, null, msg3.getBytes()); // 5.关闭资源 channel.close(); connection.close(); } } 生产者运行 消费者控制台输出如下 可见,在交换机类型为topic模式时,RabbitMQ的交换机根据生产者发送消息的路由键将接受到的消息根据topic模式的路由键路由到相应的队列中,并被相应的监听消费者消费该消息.(注意通配符的模糊匹配) Fanout Exchange 不处理路由键,只需要简单的将队列绑定到交换机上 发送到交换机的消息都会被转发到与该交换机绑定的所有队列上 Fanout交换机转发消息是最快的以下为Fanout模式下Exchange的工作示意图: Fanout Exchange演示消费者代码 消费者代码如下 import com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; import com.rabbitmq.client.QueueingConsumer; /** * @Classname Consumer * @Description RabbitMQ的Fanout模式:消费者 * @Date 2019/9/3 17:14 * @Created by Jiavg */ public class Consumer { public static void main(String[] args) throws Exception{ // 1.创建ConnectionFactory ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(\"127.0.0.1\"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost(\"/\"); connectionFactory.setUsername(\"guest\"); connectionFactory.setPassword(\"guest\"); // 2.通过连接工厂获得连接 Connection connection = connectionFactory.newConnection(); // 3.通过连接创建信道 Channel channel = connection.createChannel(); String exchangeName = \"test_Fanout_Exchange\"; String exchangeType = \"fanout\"; String queueName = \"test_Fanout_Queue\"; String routeKey = \"\"; channel.exchangeDeclare(exchangeName, exchangeType, true, false,false, null); channel.queueDeclare(queueName, true, false, false, null); channel.queueBind(queueName, exchangeName, routeKey); QueueingConsumer queueingConsumer = new QueueingConsumer(channel); channel.basicConsume(queueName, true, queueingConsumer); while (true) { QueueingConsumer.Delivery delivery = queueingConsumer.nextDelivery(); byte[] body = delivery.getBody(); String msg = new String(body); System.out.printf(msg); } } } 消费者运行 在运行消费者后,RabbitMQ的管控台变化如下: Exchanges视口 由上图可以观察到,声明的交换机(“test_Fanout_Queue”)为fanout模式. 生产者代码 生产者代码如下 import com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; /** * @Classname Producer * @Description RabbitMQ的Fanout模式:生产者 * @Date 2019/9/3 22:14 * @Created by Jiavg */ public class Producer { public static void main(String[] args) throws Exception{ // 1.创建ConnectionFactory ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(\"127.0.0.1\"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost(\"/\"); connectionFactory.setUsername(\"guest\"); connectionFactory.setPassword(\"guest\"); // 2.通过连接工厂获得连接 Connection connection = connectionFactory.newConnection(); // 3.通过连接创建信道 Channel channel = connection.createChannel(); String exchangeName = \"test_Fanout_Exchange\"; String routeKey = \"\"; String msg = \"Hello World Fanout Exchange\"; channel.basicPublish(exchangeName, routeKey, null, msg.getBytes()); } } 生产者运行 消费者控制台输出如下 可见,在交换机类型为fanout模式时,RabbitMQ不处理路由键,只需要简单的将队列绑定到交换机上,发送到交换机的消息都会被转发到与该交换机绑定的所有队列上.","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/categories/消息中间件/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://lylgjiavg.github.io/tags/RabbitMQ/"},{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/tags/消息中间件/"}]},{"title":"RabbitMQ快速入门","slug":"RabbitMQ03","date":"2019-09-02T04:17:03.000Z","updated":"2019-09-07T02:44:37.277Z","comments":true,"path":"2019/09/02/RabbitMQ03/","link":"","permalink":"https://lylgjiavg.github.io/2019/09/02/RabbitMQ03/","excerpt":"","text":"RabbitMQ入门RabbitMQ 中 Connection 和 Channel 我们知道无论是生产者还是消费者，都需要和 RabbitMQ Broker 建立连接，这个连接就是一条 TCP 连接，也就是 Connection。 一旦 TCP 连接建立起来，客户端紧接着可以创建一个 AMQP 信道（Channel），每个信道都会被指派一个唯一的 ID。 信道是建立在 Connection 之上的虚拟连接，RabbitMQ 处理的每条 AMQP 指令都是通过信道完成的。 我们完全可以使用 Connection 就能完成信道的工作，为什么还要引入信道呢？ 试想这样一个场景，一个应用程序中有很多个线程需要从 RabbitMQ 中消费消息，或者生产消息，那么必然需要建立很多个 Connection，也就是多个 TCP 连接。然而对于操作系统而言，建立和销毁 TCP 连接是非常昂贵的开销，如果遇到使用高峰，性能瓶颈也随之显现。 RabbitMQ 采用类似 NIO（Non-blocking I/O）的做法，选择 TCP 连接复用，不仅可以减少性能开销，同时也便于管理。 每个线程把持一个信道，所以信道复用了 Connection 的 TCP 连接。同时 RabbitMQ 可以确保每个线程的私密性，就像拥有独立的连接一样。当每个信道的流量不是很大时，复用单一的 Connection 可以在产生性能瓶颈的情况下有效地节省 TCP 连接资源。但是信道本身的流量很大时，这时候多个信道复用一个Connection 就会产生性能瓶颈，进而使整体的流量被限制了。此时就需要开辟多个 Connection，将这些信道均摊到这些 Connection 中，至于这些相关的调优策略需要根据业务自身的实际情况进行调节。 信道在 AMQP 中是一个很重要的概念，大多数操作都是在信道这个层面展开的。比如 channel.exchangeDeclare、channel.queueDeclare、channel.basicPublish、channel.basicConsume 等方法。RabbitMQ 相关的 API 与 AMQP 紧密相连，比如 channel.basicPublish 对应 AMQP 的 Basic.Publish 命令。 名词解释：NIO，也称非阻塞 I/O，包含三大核心部分：Channel（信道）、Buffer（缓冲区）和 Selector（选择器）。NIO 基于 Channel 和 Buffer 进行操作，数据总是从信道读取数据到缓冲区中，或者从缓冲区写入到信道中。Selector 用于监听多个信道的时间（比如连接打开，数据到达等）。因此，单线程可以监听多个数据的信道。 创建Java项目在pom文件中引入RabbitMQ依赖项: &lt;!-- https://mvnrepository.com/artifact/com.rabbitmq/amqp-client --> &lt;dependency> &lt;groupId>com.rabbitmq&lt;/groupId> &lt;artifactId>amqp-client&lt;/artifactId> &lt;version>3.6.5&lt;/version> &lt;/dependency> 创建消费者import com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; import com.rabbitmq.client.QueueingConsumer; /** * @Classname Consumer * @Description RabbitMQ消费者 * @Date 2019/9/2 11:02 * @Created by Jiavg */ public class Consumer { public static void main(String[] args) throws Exception { // 1.创建一个ConnectionFactory,并进行配置 ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(\"127.0.0.1\"); connectionFactory.setUsername(\"guest\"); connectionFactory.setPassword(\"guest\"); connectionFactory.setVirtualHost(\"/\"); // 2.通过连接工厂创建连接 Connection connection = connectionFactory.newConnection(); // 3.通过connection创建一个Channel Channel channel = connection.createChannel(); // 4.声明(创建)一个队列 String queueName = \"test001\"; // queueDeclare(String queue, boolean durable, boolean exclusive, boolean autoDelete,Map&lt;String, Object> arguments) // param queue the name of the queue // @param durable true if we are declaring a durable queue (the queue will survive a server restart) // @param exclusive true if we are declaring an exclusive queue (restricted to this connection) // @param autoDelete true if we are declaring an autodelete queue (server will delete it when no longer in use) // @param arguments other properties (construction arguments) for the queue channel.queueDeclare(queueName, true, false,false,null); // 5.创建消费者 QueueingConsumer queueingConsumer = new QueueingConsumer(channel); // 6.设置Channel channel.basicConsume(queueName, true, queueingConsumer); // 7.获取消息 while (true){ QueueingConsumer.Delivery delivery = queueingConsumer.nextDelivery(); byte[] body = delivery.getBody(); String msg = new String(body); System.out.println(\"消费者:\" + msg); } } } 运行消费者类 在运行消费者类Consumer后,RabbitMQ的浏览器管控台信息状况如下图所示: 运行消费者类之前: 由上图可以看到: 在全局参数(Global counts)中:连接数量(Connections)为0,信道(Channels)数量为0,队列(Queues)数量为0,以及消费者(Consumers)数量为0 运行消费者之后由上图可以看到: 在全局参数(Global counts)中:连接数量(Connections)为1,信道(Channels)数量为1,队列(Queues)数量为1,以及消费者(Consumers)数量为1. 可以了解到: channel.queueDeclare(queueName, true, false,false,null);此行代码创建了一个名字为”test001”(即queueName变量值)的队列,且与RabbitMQ保持一个连接,此连接内包含一个信道. QueueingConsumer queueingConsumer = new QueueingConsumer(channel);channel.basicConsume(queueName, true, queueingConsumer);这两行代码创建一个消费者,并把消费者绑定到已创建的队列上. 创建生产者类import com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; /** * @Classname Producer * @Description RabbitMQ生产者 * @Date 2019/9/2 10:54 * @Created by Jiavg */ public class Producer { public static void main(String[] args) throws Exception { // 1.创建一个ConnectionFactory,并进行配置 ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(\"127.0.0.1\"); connectionFactory.setUsername(\"guest\"); connectionFactory.setPassword(\"guest\"); connectionFactory.setVirtualHost(\"/\"); // 2.通过连接工厂创建连接 Connection connection = connectionFactory.newConnection(); // 3.通过connection创建一个Channel Channel channel = connection.createChannel(); for (int i = 0; i &lt; 5; i++){ String message = \"Hello RabbitMQ!\"; // 4.通过Channel发送数据 // basicPublish(String exchange, String routingKey, BasicProperties props, byte[] body) channel.basicPublish(\"\", \"test001\", null, message.getBytes()); } // 5.关闭相关连接 channel.close(); connection.close();; } } 运行生产者类 在运行生产者类Producer后,RabbitMQ的浏览器管控台信息状况如下图所示: 运行生产者类之前: 由上图可以看到: 消息队列(Queued messages)为空,消息消费速率(Message rates)也为0 运行生产者之后 且消费者控制台显示: 由上图可以看到: 消息队列(Queued messages)为空,消息消费速率(Message rates)迅速上升并下降 消费者接收到生产者发送的信息 以上便是RabbitMQ快速入门.","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/categories/消息中间件/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://lylgjiavg.github.io/tags/RabbitMQ/"},{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/tags/消息中间件/"}]},{"title":"RabbitMQ命令行与管控台","slug":"RabbitMQ02","date":"2019-09-02T00:15:36.000Z","updated":"2019-09-04T07:29:46.747Z","comments":true,"path":"2019/09/02/RabbitMQ02/","link":"","permalink":"https://lylgjiavg.github.io/2019/09/02/RabbitMQ02/","excerpt":"","text":"命令行与管控台-基础操作 启动应用 [root@Jiavg /]# rabbitmqctl start_app Starting node rabbit@Jiavg … 节点状态 [root@Jiavg /]# rabbitmqctl status Status of node rabbit@Jiavg ... [{pid,24387}, {running_applications, [{rabbitmq_management,&quot;RabbitMQ Management Console&quot;,&quot;3.6.6&quot;}, {rabbitmq_management_agent,&quot;RabbitMQ Management Agent&quot;,&quot;3.6.6&quot;}, {rabbitmq_web_dispatch,&quot;RabbitMQ Web Dispatcher&quot;,&quot;3.6.6&quot;}, {rabbit,&quot;RabbitMQ&quot;,&quot;3.6.6&quot;}, {os_mon,&quot;CPO CXC 138 46&quot;,&quot;2.4.1&quot;}, {amqp_client,&quot;RabbitMQ AMQP Client&quot;,&quot;3.6.6&quot;}, {rabbit_common,[],&quot;3.6.6&quot;}, {webmachine,&quot;webmachine&quot;,&quot;1.10.3&quot;}, {mochiweb,&quot;MochiMedia Web Server&quot;,&quot;2.13.1&quot;}, {mnesia,&quot;MNESIA CXC 138 12&quot;,&quot;4.14&quot;}, {inets,&quot;INETS CXC 138 49&quot;,&quot;6.3.2&quot;}, {ssl,&quot;Erlang/OTP SSL application&quot;,&quot;8.0.1&quot;}, {public_key,&quot;Public key infrastructure&quot;,&quot;1.2&quot;}, {xmerl,&quot;XML parser&quot;,&quot;1.3.11&quot;}, {crypto,&quot;CRYPTO&quot;,&quot;3.7&quot;}, {compiler,&quot;ERTS CXC 138 10&quot;,&quot;7.0.1&quot;}, {asn1,&quot;The Erlang ASN1 compiler version 4.0.3&quot;,&quot;4.0.3&quot;}, {ranch,&quot;Socket acceptor pool for TCP protocols.&quot;,&quot;1.2.1&quot;}, {syntax_tools,&quot;Syntax tools&quot;,&quot;2.0&quot;}, {sasl,&quot;SASL CXC 138 11&quot;,&quot;3.0&quot;}, {stdlib,&quot;ERTS CXC 138 10&quot;,&quot;3.0.1&quot;}, {kernel,&quot;ERTS CXC 138 10&quot;,&quot;5.0.1&quot;}]}, {os,{unix,linux}}, {erlang_version, &quot;Erlang/OTP 19 [erts-8.0.3] [source] [64-bit] [smp:2:2] [async-threads:64] [hipe] [kernel-poll:true]\\n&quot;}, {memory, [{total,55645528}, {connection_readers,0}, {connection_writers,0}, {connection_channels,0}, {connection_other,2832}, {queue_procs,2832}, {queue_slave_procs,0}, {plugins,770696}, {other_proc,18182848}, {mnesia,67616}, {mgmt_db,661496}, {msg_index,44080}, {other_ets,1430128}, {binary,29576}, {code,24640713}, {atom,1000625}, {other_system,8812086}]}, {alarms,[]}, {listeners,[{clustering,25672,&quot;::&quot;},{amqp,5672,&quot;::&quot;}]}, {vm_memory_high_watermark,0.4}, {vm_memory_limit,767174246}, {disk_free_limit,50000000}, {disk_free,20552192000}, {file_descriptors, [{total_limit,924},{total_used,2},{sockets_limit,829},{sockets_used,0}]}, {processes,[{limit,1048576},{used,228}]}, {run_queue,0}, {uptime,80}, {kernel,{net_ticktime,60}}] 关闭应用 [root@Jiavg /]# rabbitmqctl stop_app Stopping node rabbit@Jiavg … 注意: 如果在关闭RabbitMQ管控台(即使用rabbitmqctl stop_app命令)后,重新启动管控台,可能会出现如下情况: [root@Jiavg /]# rabbitmqctl start_app Starting node rabbit@Jiavg ... Error: unable to connect to node rabbit@Jiavg: nodedown DIAGNOSTICS =========== attempted to contact: [rabbit@Jiavg] rabbit@Jiavg: * connected to epmd (port 4369) on Jiavg * epmd reports: node &#39;rabbit&#39; not running at all no other nodes on Jiavg * suggestion: start the node current node details: - node name: &#39;rabbitmq-cli-99@Jiavg&#39; - home dir: /var/lib/rabbitmq - cookie hash: aTORW4S5X/Uol6/P8PSLqw== 解决方案: 1.kill掉RabbitMQ所有进程 2.重启RabbitMQ服务 [root@Jiavg /]# rabbitmq-server start RabbitMQ 3.6.6. Copyright (C) 2007-2016 Pivotal Software, Inc. ## ## Licensed under the MPL. See http://www.rabbitmq.com/ ## ## ########## Logs: /var/log/rabbitmq/rabbit@Jiavg.log ###### ## /var/log/rabbitmq/rabbit@Jiavg-sasl.log ########## Starting broker... completed with 6 plugins. 3.重新运行rabbitmqctl start_app命令 使用lsof命令查看RabbitMQ状态 lsof(list open files)是一个列出当前系统打开文件的工具。在linux环境下，任何事物都以文件的形式存在，通过文件不仅仅可以访问常规数据，还可以访问网络连接和硬件。所以如传输控制协议 (TCP) 和用户数据报协议 (UDP) 套接字等，系统在后台都为该应用程序分配了一个文件描述符，无论这个文件的本质如何，该文件描述符为应用程序与基础操作系统之间的交互提供了通用接口。因为应用程序打开文件的描述符列表提供了大量关于这个应用程序本身的信息，因此通过lsof工具能够查看这个列表对系统监测以及排错将是很有帮助的。 [root@Jiavg /]# lsof -i:5672 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME beam.smp 24387 rabbitmq 53u IPv6 139726 0t0 TCP *:amqp (LISTEN) 添加用户 rabbitmqctl add_user username password [root@Jiavg /]# rabbitmqctl add_user jiavg jiavg Creating user “jiavg” … 列出所有用户 rabbitmqctl list_users [root@Jiavg /]# rabbitmqctl list_users Listing users … jiavg [] guest [administrator] 修改密码 rabbitmqctl change_password username newpassword [root@Jiavg /]# rabbitmqctl change_password jiavg jiavg123 Changing password for user “jiavg” .. 删除用户 rabbitmqctl delete_user username [root@Jiavg /]# rabbitmqctl delete_user jiavg Deleting user “jiavg” … 创建虚拟主机 *rabbitmqctl add_vhost *vhostpath ** [root@Jiavg /]# rabbitmqctl add_vhost /test Creating vhost “/test” … 列出所有虚拟主机 rabbitmqctl list_vhosts [root@Jiavg /]# rabbitmqctl list_vhosts Listing vhosts … /test / 设置用户权限 rabbitmqctl set_permissions -p vhostpath username “.“ “.*” “.*”* [root@Jiavg /]# rabbitmqctl set_permissions -p /test jiavg “.*” “.*” “.*” Setting permissions for user “jiavg” in vhost “/test” … 列出虚拟主机上所有权限 rabbitmqctl list_permissions -p vhostpath [root@Jiavg /]# rabbitmqctl list_permissions -p /test Listing permissions in vhost “/test” … jiavg .* .* .* 清除用户权限 rabbitmqctl clear_permissions -p vhostpath username [root@Jiavg /]# rabbitmqctl clear_permissions -p /test jiavg Clearing permissions for user “jiavg” in vhost “/test” … 查看所有交换机信息 rabbitmqctl list_exchanges [root@Jiavg /]# rabbitmqctl list_exchanges Listing exchanges … direct amq.fanout fanout amq.headers headers amq.rabbitmq.log topic amq.direct direct amq.match headers amq.rabbitmq.trace topic amq.topic topic 查看所有队列信息 rabbitmqctl list_queues [root@Jiavg /]# rabbitmqctl list_queues Listing queues … 注:此时没有队列 清除队列里的消息 rabbitmqctl -p vhostpath purge_queue blue 命令行与管控台-高级操作 移除所有数据,要在rabbitmqctl stop_app之后使用 rabbitmqctl reset 组成集群命令 rabbitmqctl join_cluster [–ram] 查看集群状态 rabbitmqctl cluster_status 修改集群节点的储存形式 rabbitmqctl change_cluster_node_type disc|ram 忘记节点(摘除节点) rabbitmqctl forget_cluster_node [–offline] 修改节点名称 rabbitmqctl rename_cluster_node oldnode1 newnode1 [oldnode2] [newnode2…] 管控台-Overview项预览","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/categories/消息中间件/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://lylgjiavg.github.io/tags/RabbitMQ/"},{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/tags/消息中间件/"}]},{"title":"RabbitMQ安装及基本设置","slug":"RabbitMQ安装01","date":"2019-09-01T02:27:36.000Z","updated":"2019-09-02T02:27:19.317Z","comments":true,"path":"2019/09/01/RabbitMQ安装01/","link":"","permalink":"https://lylgjiavg.github.io/2019/09/01/RabbitMQ安装01/","excerpt":"","text":"安装环境摘要为了完整模拟RabbitMQ安装的从无到有,本次安装使用了VMware克隆了一个初始状态下的CentOS7镜像,下面介绍一下安装前的准备工作建议配置 更换阿里巴巴的yum仓库,便于下载所需文件: CentOS 1、备份 mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup 2、下载新的CentOS-Base.repo 到/etc/yum.repos.d/ CentOS 7 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 或者 curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 3、之后运行yum makecache生成缓存 具体可参考阿里巴巴镜像站 更换主机名 进入主机名配置文件,并修改主机名(在安装RabbitMQ后RabbitMQ会使用主机名,建议配置) *vim /etc/hostname * 本机修改为Jiavg RabbitMQ安装步骤1.创建下载文件所需的文件夹 [root@Jiavg /]# mkdir -p /usr/local/software/ [root@Jiavg /]# cd /usr/local/software/ [root@Jiavg software]# 2.下载所需文件 下载erlang和rabbitmq-server的rpm: [root@Jiavg software]#wget http://www.rabbitmq.com/releases/erlang/erlang-19.0.4-1.el7.centos.x86_64.rpm [root@Jiavg software]#wget http://www.rabbitmq.com/releases/rabbitmq-server/v3.6.6/rabbitmq-server-3.6.6-1.el7.noarch.rpm 验证文件是否下载成功 [root@Jiavg software]# ls erlang-19.0.4-1.el7.centos.x86_64.rpm rabbitmq-server-3.6.6-1.el7.noarch.rpm 3.安装Erlang 安装erlang [root@Jiavg software]#rpm -ivh erlang-19.0.4-1.el7.centos.x86_64.rpm 验证erlang是否安装成功 [root@Jiavg software]# erl Erlang/OTP 19 [erts-8.0.3] [source] [64-bit] [smp:2:2] [async-threads:10] [hipe] [kernel-poll:false] Eshell V8.0.3 (abort with ^G) 1&gt; 4.安装RabbitMQ 安装socat(安装RabbitMQ需要此命令进行密匙校验) [root@Jiavg software]# yum install socat 如果不安装socat,则会出现以下情况: [root@Jiavg software]# rpm -ivh rabbitmq-server-3.6.6-1.el7.noarch.rpm 警告：rabbitmq-server-3.6.6-1.el7.noarch.rpm: 头V4 RSA/SHA1 Signature, 密钥 ID 6026dfca: NOKEY 错误：依赖检测失败： socat 被 rabbitmq-server-3.6.6-1.el7.noarch 需要 安装RabbitMQ [root@Jiavg software]# rpm -ivh rabbitmq-server-3.6.6-1.el7.noarch.rpm 警告：rabbitmq-server-3.6.6-1.el7.noarch.rpm: 头V4 RSA/SHA1 Signature, 密钥 ID 6026dfca: NOKEY 准备中… ################################# [100%] 正在升级/安装… 1:rabbitmq-server-3.6.6-1.el7 ################################# [100%] 5.修改基本配置文件 进入RabbitMQ配置文件目录 [root@Jiavg ebin]# cd /usr/lib/rabbitmq/lib/rabbitmq_server-3.6.6/ebin/ 修改rabbit.app文件 [root@Jiavg ebin]#vim rabbit.app 在vim中使用/loopback定位需要修改的文件位置 /loopback 文件被定位到 {loopback_users, [&lt;&lt;”guest”&gt;&gt;]}, 修改为 {loopback_users, [guest]}, 即:删除&lt;&lt;”和”&gt;&gt; 保存并退出 6.安装RabbitMQ的rabbitmq_management插件 RabbitMQ的rabbitmq_management插件可以通过浏览器的可视化界面来管理RabbitMQ 启动RabbitMQ [root@Jiavg ebin]# rabbitmq-server start &amp; [1] 9843 [root@Jiavg ebin]# RabbitMQ 3.6.6. Copyright (C) 2007-2016 Pivotal Software, Inc. ## ## Licensed under the MPL. See http://www.rabbitmq.com/ ## ## ########## Logs: /var/log/rabbitmq/rabbit@Jiavg.log ###### ## /var/log/rabbitmq/rabbit@Jiavg-sasl.log ########## Starting broker... 查看本地可用插件 [root@Jiavg ebin]# rabbitmq-plugins list Configured: E = explicitly enabled; e = implicitly enabled | Status: [failed to contact rabbit@Jiavg - status not shown] |/ [ ] amqp_client 3.6.6 [ ] cowboy 1.0.3 [ ] cowlib 1.0.1 [ ] mochiweb 2.13.1 [ ] rabbitmq_amqp1_0 3.6.6 [ ] rabbitmq_auth_backend_ldap 3.6.6 [ ] rabbitmq_auth_mechanism_ssl 3.6.6 [ ] rabbitmq_consistent_hash_exchange 3.6.6 [ ] rabbitmq_event_exchange 3.6.6 [ ] rabbitmq_federation 3.6.6 [ ] rabbitmq_federation_management 3.6.6 [ ] rabbitmq_jms_topic_exchange 3.6.6 [ ] rabbitmq_management 3.6.6 [ ] rabbitmq_management_agent 3.6.6 [ ] rabbitmq_management_visualiser 3.6.6 [ ] rabbitmq_mqtt 3.6.6 [ ] rabbitmq_recent_history_exchange 1.2.1 [ ] rabbitmq_sharding 0.1.0 [ ] rabbitmq_shovel 3.6.6 [ ] rabbitmq_shovel_management 3.6.6 [ ] rabbitmq_stomp 3.6.6 [ ] rabbitmq_top 3.6.6 [ ] rabbitmq_tracing 3.6.6 [ ] rabbitmq_trust_store 3.6.6 [ ] rabbitmq_web_dispatch 3.6.6 [ ] rabbitmq_web_stomp 3.6.6 [ ] rabbitmq_web_stomp_examples 3.6.6 [ ] sockjs 0.3.4 [ ] webmachine 1.10.3 安装rabbitmq_management插件 [root@Jiavg ebin]# rabbitmq-plugins enable rabbitmq_management 7.测试RabbitMQ的rabbitmq_management插件 使用浏览器访问 IP地址:15672 把IP地址替换为你的RabbitMQ安装的主机IP 注意:如果不能正常访问,可能是防火墙问题 此时需要开启15672端口的访问权限 如以上步骤都正确,则浏览器会显示以下页面 在对应的输入框输入如下 Username:guest Password:guest 即:用户名和密码都是guest,这是RabbitMQ默认的账号和密码 在成功输入账户和密码后会进入如下界面 此时,RabbitMQ已经安装完成.","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/categories/消息中间件/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://lylgjiavg.github.io/tags/RabbitMQ/"},{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/tags/消息中间件/"}]},{"title":"RabbitMQ特点","slug":"RabbitMQ","date":"2019-08-31T23:48:15.000Z","updated":"2019-09-02T02:29:36.332Z","comments":true,"path":"2019/09/01/RabbitMQ/","link":"","permalink":"https://lylgjiavg.github.io/2019/09/01/RabbitMQ/","excerpt":"","text":"初识RabbitMQRabbitMQ是一个开源的消息代理和队列服务器,用来通过普通协议在完全不同的应用之间共享数据,RabbitMQ是使用Erlang语言来编写的,并且RabbitMQ是基于AMQP协议的.RabbitMQ优点 开源,性能优秀,稳定性保障 提供可靠性消息投递模式(confirm),返回模式(return) 与SpringAMQP完美整合,API丰富 集群模式丰富,表达式配置,HA(High Available)模式,镜像队列模型 保证数据不丢失的前提下做到高可靠性,可用性 RabbitMQ高性能原因? Erlang语言最初在于交换机领域的架构模式,这样使得RabbitMQ在Broker之间进行数据交互的性能是非常优秀的 Erlang优点:Erlang和原生Socket一样的延迟 什么是AMQP高级消息队列协议? AMQP全称:Advanced Message Queuing Protocol AMQP定义:是具有现代特征的二进制协议.是一个提供统一消息服务的应用层标准高级消息队列协议,是应用层协议的一个开放标准,为面向消息的中间件设计. AMQP核心概念 Server:又称Broker,接受客户端的连接,实现AMQP实体服务 Connection:连接,应用程序与Broker的网络连接 Channel:网络信道,几乎所有的操作都在Channel在进行,Channel是进行消息的读写的通道.客户端可建立多个Channel,每个Channel代表一个会话任务. Message:消息,服务器与应用程序之间传送的数据,由Properties和Body组成.Properties可以对消息进行修饰,比如消息的优先级,延迟等高级特性;Body则就是消息体内容. Virtual host:虚拟地址,用于进行逻辑分离,最上层的消息路由.一个Virtual Host里面可以有若干个Exchange和Queue,同一个Virtual Host里面不能有相同名称的Exchange或Queue. Exchange:交换机,接收消息,根据路由键转发消息到绑定的队列. Binding:Exchange和Queue之间的虚拟连接,binding中可以包含routing key Routing key:一个路由规则,虚拟机可以用它来确定如何路由一个特定消息 Queue:也称Message Queue,消息队列,保存消息并将它们转发给消费者. RabbitMQ架构图","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/categories/消息中间件/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://lylgjiavg.github.io/tags/RabbitMQ/"},{"name":"消息中间件","slug":"消息中间件","permalink":"https://lylgjiavg.github.io/tags/消息中间件/"}]},{"title":"牛客网刷题01","slug":"NewCoder-19-8-20","date":"2019-08-20T12:43:14.000Z","updated":"2019-09-02T02:28:01.934Z","comments":true,"path":"2019/08/20/NewCoder-19-8-20/","link":"","permalink":"https://lylgjiavg.github.io/2019/08/20/NewCoder-19-8-20/","excerpt":"","text":"1.若在某一个类定义中定义有如下的方法： abstract void performDial( ); 该方法属于（） A.本地方法 B.最终方法 C.解态方法 D.抽象方法 正确答案: D 解析: 本地方法：简单地讲，一个native Method就是一个java调用非java代码的接口；native方法表示该方法要用另外一种依赖平台的编程语言实现。 最终方法：final void B(){},这样定义的方法就是最终方法，最终方法在子类中不可以被重写，也就是说，如果有个子类继承了这个最终方法所在的类，那么这个子类中不能出现void B(){}这样的方法。 最终类：final class A {},这样定义的类就是最终类，最终类不能被继承。 abstract修饰抽象类 2.当使包含 applet 程序的页面从最小化恢复时，以下选项中的哪个方法将被执行？（ ） A.paint() B.start() C.destroy() D.stop() 正确答案: A 解析: applet页面刚打开时,程序调用init(),然后调用start(),再然后paint(); 用户离开applet页面,程序自动调用stop(),用户关闭浏览器,程序触发destroy(); 并且paint()在每一次浏览器显示页面时被调用; 最小化点开-->页面重新显示，之前进程没丢，只有paint()。 3.下列关于修饰符混用的说法，错误的是( ) A.abstract不能与final并列修饰同一个类 B.abstract类中不应该有private的成员 C.abstract方法必须在abstract类或接口中 D.static方法中能直接调用类里的非static的属性","categories":[{"name":"nowcoder","slug":"nowcoder","permalink":"https://lylgjiavg.github.io/categories/nowcoder/"}],"tags":[{"name":"牛客网","slug":"牛客网","permalink":"https://lylgjiavg.github.io/tags/牛客网/"}]},{"title":"Docker基本使用","slug":"Docker","date":"2019-08-17T12:09:32.000Z","updated":"2019-08-17T16:04:53.304Z","comments":true,"path":"2019/08/17/Docker/","link":"","permalink":"https://lylgjiavg.github.io/2019/08/17/Docker/","excerpt":"","text":"Docker描述 Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的镜像中，然后发布到 任何流行的 Linux或Windows 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。Docker安装进入yum目录 [root@bogon yum.repos.d]cd /etc/yum.repos.d/ 下载repo [root@bogon yum.repos.d]# wget https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repo --2019-08-17 21:23:14-- https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repo 正在解析主机 mirrors.tuna.tsinghua.edu.cn (mirrors.tuna.tsinghua.edu.cn)... 101.6.8.193, 2402:f000:1:408:8100::1 正在连接 mirrors.tuna.tsinghua.edu.cn (mirrors.tuna.tsinghua.edu.cn)|101.6.8.193|:443... 已连接。 已发出 HTTP 请求，正在等待回应... 200 OK 长度：2424 (2.4K) [application/octet-stream] 正在保存至: “docker-ce.repo” 100%[==========================================================================================================>] 2,424 --.-K/s 用时 0s 2019-08-17 21:23:14 (262 MB/s) - 已保存 “docker-ce.repo” [2424/2424]) 验证docker-ce是否安装成功 [root@bogon yum.repos.d]# ls CentOS-Base.repo CentOS-Debuginfo.repo CentOS-Media.repo CentOS-Vault.repo CentOS-CR.repo CentOS-fasttrack.repo CentOS-Sources.repo docker-ce.repo [root@bogon yum.repos.d]# yum repolist 已加载插件：fastestmirror, langpacks Loading mirror speeds from cached hostfile * base: mirrors.njupt.edu.cn * extras: mirrors.cn99.com * updates: mirrors.cn99.com 源标识 源名称 状态 base/7/x86_64 CentOS-7 - Base 10,019 docker-ce-stable/x86_64 Docker CE Stable - x86_64 52 extras/7/x86_64 CentOS-7 - Extras 435 updates/7/x86_64 CentOS-7 - Updates 2,500 repolist: 13,006 [root@bogon yum.repos.d]# docker --version Docker version 19.03.1, build 74b1e89 配置Docker加速器Json[root@bogon yum.repos.d]# mkdir /etc/docker/ [root@bogon yum.repos.d]# vi /etc/docker/daemon.json 在daemon.json 中添加 { &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;] } 测试Docker创建一个httpd服务获得busybox镜像 [root@bogon yum.repos.d]# docker image pull busybox 利用busybox创建一个容器 [root@bogon yum.repos.d]# docker container run --name myhttpd -it busybox 创建httpd工作目录及index.html / # mkdir /data/html -p / # vi /data/html/index.html 在index.html添加&quot;Hello Docker!&quot; 此时此进程处于阻塞状态,然后再启动另一个Shell进程 使用docker inspect myhttpd命令查看myhttpd容器的状态信息 [root@bogon ~]# docker inspect myhttpd [ { ..., ..., &quot;NetworkSettings&quot;: { ..., ... &quot;Networks&quot;: { &quot;bridge&quot;: { &quot;IPAMConfig&quot;: null, &quot;Links&quot;: null, &quot;Aliases&quot;: null, &quot;NetworkID&quot;: &quot;c28b3cad37536a90abf27043b6b2caa931bcd31ec79b0785d92ee00236a8f575&quot;, &quot;EndpointID&quot;: &quot;7dedddc84a36d48c4d6d78773cb571179bd455704cb5c341e6fa43764a2dac0d&quot;, &quot;Gateway&quot;: &quot;172.17.0.1&quot;, &quot;IPAddress&quot;: &quot;172.17.0.4&quot;, &quot;IPPrefixLen&quot;: 16, &quot;IPv6Gateway&quot;: &quot;&quot;, &quot;GlobalIPv6Address&quot;: &quot;&quot;, &quot;GlobalIPv6PrefixLen&quot;: 0, &quot;MacAddress&quot;: &quot;02:42:ac:11:00:04&quot;, &quot;DriverOpts&quot;: null } } } } ] 在NetworkSettings下的IPAddress可以获得myhttpd的容器虚拟地址为172.17.0.4 此时使用curl命令来验证httpd服务是否进行 [root@bogon ~]# curl 172.17.0.4 Hello Docker! 可以观察到myhttpd容器内的httpd服务正常运行附:Docker生命周期","categories":[{"name":"容器引擎","slug":"容器引擎","permalink":"https://lylgjiavg.github.io/categories/容器引擎/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://lylgjiavg.github.io/tags/Docker/"},{"name":"container","slug":"container","permalink":"https://lylgjiavg.github.io/tags/container/"}]},{"title":"ArrayList","slug":"ArrayList","date":"2019-08-14T14:28:50.000Z","updated":"2019-08-14T14:57:40.566Z","comments":true,"path":"2019/08/14/ArrayList/","link":"","permalink":"https://lylgjiavg.github.io/2019/08/14/ArrayList/","excerpt":"","text":"ArrayList类继承关系 IDEA使用Ctrl+Shift+Alt+N快捷键查找此类 ArrayList类继承关系如下图所示: ![ArrayList继承关系图](https://i.imgur.com/nTyDgMV.jpg)Iterable接口 Implementing this interface allows an object to be the target of the “for-each loop” statement. 即:实现这个接口允许对象成为“for-each循环”语句的目标。 而java.util.Collection接口继承java.lang.Iterable，故标准类库中的任何集合都可以使用for-each循环。 为什么一定要去实现Iterable这个接口呢？ 为什么不直接实现Iterator接口呢？看一下JDK中的集合类，比如List一族或者Set一族，都是继承了Iterable接口，但并不直接继承Iterator接口。仔细想一下这么做是有道理的。因为Iterator接口的核心方法next()或者hasNext()是依赖于迭代器的当前迭代位置的。如果Collection直接继承Iterator接口，势必导致集合对象中包含当前迭代位置的数据(指针)。当集合在不同方法间被传递时，由于当前迭代位置不可预置，那么next()方法的结果会变成不可预知。除非再为Iterator接口添加一个reset()方法，用来重置当前迭代位置。但即时这样，Collection也只能同时存在一个当前迭代位置。而Iterable则不然，每次调用都会返回一个从头开始计数的迭代器。多个迭代器是互不干扰的。","categories":[{"name":"数据结构和算法","slug":"数据结构和算法","permalink":"https://lylgjiavg.github.io/categories/数据结构和算法/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://lylgjiavg.github.io/tags/数据结构/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-06-29T12:51:02.577Z","updated":"2019-06-29T12:51:02.577Z","comments":true,"path":"2019/06/29/hello-world/","link":"","permalink":"https://lylgjiavg.github.io/2019/06/29/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new \"My New Post\" More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}